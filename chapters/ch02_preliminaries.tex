\chapter{پیش‌نیازها}
\label{chapter:preliminaries}
در این فصل به مرور برخی از تعاریف و قضایای اولیه در شاخه‌های مختلف ریاضی که پیش‌نیاز مطالعه‌ی کدگذاری اندیس منعطف است، می‌پردازیم. در ابتدای هر بخش مرجعی برای مبحث مورد نظر پیشنهاد شده است، سپس مواردی که به طور قابل توجه‌ مورد نیاز هستند آورده شده‌اند.

\section{احتمال}
\begin{definition}[\transf{متغیر برنولی}{bernoulli random variable}]
	به متغیر تصادفی گسسته
	$X$
	با مقادیر
	$0$
	و
	$1$
	که تابع توزیع آن برابر
	$P(X = 1) = p = 1 - P(X = 0)$
	باشد، متغیر برنولی گویند.
\end{definition}
\begin{definition}[\transf{متغیر مستقل با توزیع یکسان}{i.i.d, Independent and identically distributed}]
به مجموعه‌ای از متغیرهای تصادفی که از هم دیگر مستقل‌‌اند و تابع توزیع یکسانی دارند، گفته می‌شود.
\end{definition}
\begin{definition}[\transf{تابع محدب}{convex function}]
	تابع
	$f: X \rightarrow \mathbb{R}$
	با مقادیر حقیقی، یک تابع محدب نامیده می‌شود اگر هر پاره خط میان دو نقطه‌ی متمایز از آن در نمودار تابع بالای تابع قرار بگیرد. به زبان ریاضی:
	$$\forall t 0 < t < 1, \forall x_1, x_2: x_1 \ne x_2: \Rightarrow f(f x_1 + (1 - t) x_2) \leq tf(x_1) + (1 - t) f(x_2)$$
\end{definition}
\begin{theorem}[\transf{نامساوی ینسن}{Jensen's inequality}]
	\label{Jensen}
	برای تابع محدب
	$f$
	داریم:
	$$f(E[X]) \leq E[f(X)]$$
	
	برای مطالعه‌ی بیشتر درباره‌ی احتمال به
	\cite{book:pro}
	مراجعه کنید.
\end{theorem}
\begin{notation}[$\log^{+}$]
	$$\log^{+}(x) = \max\{\log(x), 0\}$$
\end{notation}

\section{نظریه بازی‌ها}
موضوع مورد بحث در نظریه بازی‌ها، بررسی یک سیستم متشکل از تعدادی تصمیم‌گیر است. هر تصمیم‌گیر می‌تواند از بین تصمیمات قابل اتخاذ برای خود یک مورد را انتخاب کند. سپس بر اساس تصمیم تمام تصمیم‌گیران، سود یا ضرر هر تصمیم‌گیر مشخص می‌شود.

برای مثال فرض کنید دو نفر در حال انجام بازی سنگ کاغذ قیچی هستند. هر کدام می‌توانند یکی از سه حالت مختلف را انتخاب کنند و بر اساس انتخاب هر دو نفر یا یکی از آن‌ها پیروز شده و دیگری می‌بازد یا بازی به صورت برابر تمام می‌شود.
\begin{definition}[\transf{بهره‌وری پرتو}{Pareto efficiency}]
	\label{def:Pareto}
	به وضعیت سیستمی که تغییر انتخاب هیچ فردی نمی‌تواند سودش را افزایش دهد
	در حالی که سود بقیه افراد کاهش ندهد، گویند
	\cite{wiki:pareto}.
	\end{definition}
		\begin{definition}\transf{مرز پرتو}{Pareto boundary}
	\label{def:Pareto-boundary}
	به مجموعه نقاط فضای راه‌حل که در شرایط بهروه‌وری پرتو صدق می‌کنند گویند.
\end{definition}

	برای مطالعه‌ی بیشتر درباره‌ی نظریه‌ی بازی‌ها به
\cite{book:game}
مراجعه کنید.
\section{نظریه کدگذاری}
نظریه کدگذاری به بررسی و تحلیل کد‌ها می‌پردازد. در مخابرات و فناوری اطلاعات، یک کد، مجموعه‌‌ای از قوانین(به زبان ریاضی یک نگاشت‌) برای تبدیل داده‌ها از شکلی به شکل دیگر است، برای اهداف مختلف مانند کم‌ کردن میزان اطلاعات رد و بدل‌‌ شده برای رساندن پیام یا حفاظت از داده در برابر شنود و \dots .
از اولین نمونه‌ کدهای ساخته‌شده به دست انسان، می‌توان به زبان اشاره کرد که از مجموعه‌ای از اصوات و ترکیب آن‌ها برای انتقال پیام‌های مختلف استفاده می‌شود. به طور کلی چهار نوع کد وجود دارد \cite{wiki:coding1, wiki:coding2}:
\begin{enumerate}
	\item فشرده‌سازی منبع
	\item تصحیح خطا
	\item رمزگذاری
	\item کدگذاری خط
\end{enumerate}

در واقع موضوع این پایان‌نامه یعنی کدگذاری اندیس منعطف، خود نیز یک نوع کدگذاری است!

در این‌جا قصدی برای بررسی بیشتر نظریه کدگذاری نداریم و تنها به مرور موارد مورد نیاز خود می‌پردازیم.
\begin{example}
	فرض کنید آزاده و بردیا\footnote{دو دانشجوی دکتر محمودیان که به عنوان طرفین رمزنگاری در دانشکده شناخته می‌شوند.}
	 می‌خواهد یک رشته هفت حرفی از اعداد باینری را برای بردیا ارسال کند ولی کانال ارتباطی آن‌ها دچار مشکل است و ممکن است در هربار ارسال با احتمال
	$p$
	بیت ارسالی را به اشتباه ارسال کند. بردیا چگونه می‌تواند مطمئن شود که دقیقا پیام آلیس را دریافت کرده است؟
	
	در این‌جا با کدهای مدیریت خطا سروکار داریم. در واقع می‌خواهیم کدی بسازیم که با استفاده از آن خطای ایجاد شده هنگام ارسال پیام را متوجه شویم و گاهی حتی توانایی بازیابی پیام اصلی را از روی پیام ارسال‌شده‌ی دارای نویز داشته باشیم. در مثال بالا اگر
	$p < 2^7$
	در این صورت اگر آلیس پس از ارسال هفت بیت پیام اصلی خود، باقی‌مانده‌ی جمع بیت‌های ارسالی بر دو را نیز برای بردیا ارسال کند، پس از دریافت اطلاعات، بردیا می‌تواند بررسی کند که آیا با احتمال بالایی پیام دریافتی همان پیام ارسالی هست یا نه. به مجموعه‌ پیام‌هایی که ممکن است در فرایند کدگذاری ارسال شوند کدواژه گفته می‌شود.
	
	در مثال قبل از یک کد به شکل
	$C: \mathcal{F}_2^7 \rightarrow \mathcal{F}_2^8$
	استفاده کردیم. در این مثال
	$2^7$
	کدواژه مختلف داریم که طول هر یک
	$8$
	است.
\end{example}

\begin{definition}[\transf{فاصله‌ی همینگ}{hamming distance}
	 دو کدواژه]
	برای دو کدواژه‌ی مختلف
	$c_1, c_2 \in \mathcal{F}_q^n$
	فاصله‌ی دو کدواژه
	$d(c_1, c_2)$
	را برابر تعداد درایه‌های متفاوت بردار
	$c_1$
	با بردار
	$c_2$
	تعریف می‌کنیم.
	
	 فاصله‌‌ی کمینه مجموعه‌ی
	$C$
	از کدواژه‌های با طول
	$n$
	 را برابر
	$d(C) = \min\limits_{x, y \in C, x \ne y} d(x, y)$
	تعریف می‌کنیم.
\end{definition}

\begin{definition}[\transf{کران سینگلتون}{singleton bound}]
	اگر
	$A_q(n, d)$
	را حداکثر تعداد کدواژه‌ها در یک کد
	$n$حرفی
 از الفبای
	$\mathcal{F}_q$
	با فاصله کمینه‌ی 
	$d$
	در نظر بگیریم، داریم:
	\begin{equation}
		A_q(n, d) \leq q^{n - d + 1}
	\end{equation}
	\cite{wiki:Singleton}
\end{definition}
\begin{definition}[\transf{کد ام‌دی‌اس}{MDS code}]
	\label{def:mds}
	به کدهایی که کران سینگلتون را کسب کنند گفته می‌شود.
\end{definition}

کدهای ام‌دی‌اس بیشترین فاصله‌ی کمینه را دارند و در نتیجه بهترین کدها به عنوان کدهای مدیریت خطا هستند.
\begin{example}
	$n$
	عضو ناصفر
	$\alpha_1, \ldots, \alpha_n \in \mathcal{F}_q$
	در نظر بگیرید کد تولید شده توسط ماتریس 
	$H = (h_{ij}): h_{ij} = \alpha_j^i: i = [n - k], j = [n]$
	(که به ماتریس بررسی زوجیت معروف است.)	یک کد ام‌دی‌اس است.
\end{example}

	برای مطالعه‌ی بیشتر درباره‌ی نظریه کدگذاری به
\cite{book:coding}
مراجعه کنید.
\section{نظریه اطلاعات}
تابع آنتروپی برای نشان دادن میزان ابهام یک متغیر تصادفی تعریف می‌شود. اگر متغیر تصادفی
$x$
از مجموعه‌ی مرجع
$[n]$
به صورت یک‌نواخت انتخاب شود، تعریف می‌کنیم:
$$H(x) = \log(n)$$
در واقع آنتروپی در این حالت نشان‌دهنده‌ی تعداد بیت لازم برای این است که به فرد دیگری اطلاع دهیم مقدار نمونه متغیر تصادفی مشاهده شده چیست. برای متغیر تصادفی
$X$
با مجموعه‌ی مرجع
$[n]$
که عدد
$i$
را با احتمال
$p_i$
می‌گیرد میزان اطلاعات یا آنتروپی را به این صورت تعریف می‌کنیم:
$$H(X) = \sum_{i = 1}^{n} p_i \log(\frac{1}{n})$$

\begin{theorem}
بیشینه آنتروپی برای متغیر تصادفی که
$n$
مقدار مختلف را به خود می‌گیرد، زمانی رخ می‌دهد که
$$p_1 = p_2 = \dots p_n = \frac{1}{n}$$
\end{theorem}
\begin{proof}
برای این اثبات به نامساوی ینسن(\autoref{Jensen})
 نیاز داریم. برای تابع محدب
$f$
و نقاط
$a_1, a_2, \dots, a_n$
از دامنه‌ی آن و
$p_i \in [0, 1]$
به طوری که
$\sum_{i = 1}^{n} p_i = 1$
داریم:
$$f(\sum_{i = 1}^{n} p_i a_i) \leq \sum_{i = 1}^{n} p_i f(a_i)$$
حال اگر به حای
$f$
تابع محدب
$- log$
را قرار دهیم اثبات کامل می‌شود.
\end{proof}

\subsection{گزاره‌های ابتدایی}
\begin{theorem}
اگر
$X$
و
$Y$
دو متغیر تصادفی مستقل باشند و
$W = (X, X)$
و
$Z = (X, Y)$
داریم
\begin{align*}
    H(W) &= H(X) \\
    H(Z) &= H(X) + H(Y)
\end{align*}
\end{theorem}
\begin{definition}[آنتروپی شرطی]
	به میزان ابهام یک متغیر در صورت دانستن مقدار یک متغیر دیگر گفته می‌شود و به صورت زیر تعریف می‌شود:
$$H(X | Y = y) := \sum_{X = x} P(X = x| Y = y) \log(\frac{1}{P(X = x| Y = y)})$$
$$H(X|Y) := \sum_{Y = y} P(Y = y) H(X | Y = y)$$
\end{definition}

\begin{theorem}
اگر
$X$
و
$Y$
مستقل باشند:
$$H(X|Y) = H(X)$$
\end{theorem}
\begin{theorem}
	نامساوی‌های زیاد را می‌توان تنها با استفاده از تعریف آنتروپی به دست آورد در اینجا به چهار نامساوی اولیه که کاربردی‌ترند اشاره می‌کنیم:
	\begin{align*}
H(X) &\geq 0 \\
Z &= (X, Y) \rightarrow H(X, Y) := H(Z) \rightarrow H(X, Y) = H(Y) + H(X|Y) \\
H(X_1, X_2, \dots X_n) &= H(X_1) + H(X_2 | X_1) + \dots + H(X_n | X_1 X_2 \dots X_n)\\
H(X_1, X_2, \dots X_n) &\leq \sum_{i = 1}^{n} H(X_i)
\end{align*}
\end{theorem}
\subsection{اطلاعات متقابل}
\begin{definition}
آنتروپی نسبی یا فاصله کولبک لیبلر: برای دو تابع احتمال به شکل زیر تعریف می‌شود:
$$
D(p||q) = \sum_{x \in \mathcal{X}} p(x) \log(\dfrac{p(x)}{q(x)})
$$
که برابر است با:
$$E_p \log(\dfrac{p(X)}{q(X)})$$
\end{definition}
\begin{theorem}
آنتروپی نسبی همیشه نامنفی است و برابر صفر است اگر و تنها اگر
$p = q$
\end{theorem}
این تابع واقعا یک تابع فاصله متریک ریاضیاتی نیست ولی تفاوت بین دو توزیع را شبیه سازی می‌کند.
\begin{definition}
اطلاعات متفابل:
$$I(X; Y) := \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log(\dfrac{p(x, y)}{p(x)p(y)})$$
که برابر است با:
\begin{align*}
    I(X;Y) &= D(p(x, y) || p(x)p(y))
    \\
    &= E_{p(x, y)} \log(\dfrac{p(X, Y)}{p(X)p(Y)})
\end{align*}
\end{definition}

فرض کنید با دانستن مقدار یک نمونه از متغیر تصادفی
$Y$
مقدار نمونه متغیر تصادفی
$X$
که با
$Y$
همبسته است را حدس بزنیم. میدانیم که
$H(X|Y) = 0$
اگر و تنها اگر
$X$
تابعی از
$Y$
باشد. در قضیه‌ی زیر این مسئله به طور دقیقتر بیان شده است.
\begin{theorem}[نامساوی فانو]
فرض کنید
$P(x, y)$
تابع احتمال توام دو متغیر تصادفی باشد که
$X$
از مجموعه
$\mathcal{X}$
می‌آیند و
$Y = f(X)$
اگر تعریف کنیم
$P_e := P(X \neq \hat{X})$
و
$H_b(P_e) = - P_e \log P_e - (1 - P_e) \log(1 - P_e)$
آنگاه خواهیم داشت:
$$H(X|Y) \leq H_b(P_e) + P_e \log(\mathcal{X} - 1)$$
\end{theorem}
\begin{remark}
نامساوی فانو یک حد پایین ارائه می‌دهد که برای یک
$f$
داده شده(یک تخمین از
$X$
یا یک مشاهده نویزدار از
$X$)
هیچ تابع
$G:\mathcal{Y} \rightarrow \mathcal{X}$
(یک تخمین برای مقدار
$X$)
وجود ندارد که بهتر از این حد عمل کند و به بیان دیگر همیشه حداقل این مقدار خطا را خواهیم داشت.
\end{remark}

	برای مطالعه‌ی بیشتر درباره‌ی نظریه اطلاعات به
\cite{book:info}
مراجعه کنید.
\section{جبرخطی}
\begin{definition}
	برای مجموعه‌ی بردار‌های
	$S$، \transf{پوشش خطی}{span}،
	ترکیب خطی اعضای آن است یعنی:
	$span(S) = \{ \sum\limits_{v \in S} \lambda v\}$
	
	برای ماتریس
	$M$، $span(M)$
را پوشش خطی سطرهای 
$M$
می‌گیریم.
\end{definition}

\begin{definition}[هم‌دسته]
	\label{coset}
	برای یک زیرفضای برداری
	$V \subseteq \mathbb{F}^l$
	به
	$W$
	هم‌دسته‌‌‌ی
	$V$
	می‌گوییم اگر یک بردار
	$x \in \mathbb{F}^l$
	وجود داشته باشد که
	$$W = V + x = \{w: w \in \mathbb{F}^l, \exists v \in V, w = v + x\}$$
	هم‌دسته‌های یک زیرفضای برداری،
	\transf{زیرفضاهای آفین}{Affine subspaces}
	گفته می‌شود.
\end{definition}
\begin{definition}[\transf{هسته}{kernel}]
	هسته ماتریس
	$M$
	به صورت زیر تعریف می‌شود:
	$$ker(M) = \{v: Mv = 0\}$$
\end{definition}
برای فضای برداری
$W$،
فضای عمود یا \transf{مکمل عمود}{orthogonal complement}
 را به صورت
$W^\bot = \{v: \forall w \in W, v^T.w = 0\}$
تعریف می‌کنیم.

\begin{theorem}
	برای هر فضای برداری متناهی داریم:
	$$dim(W) + dim(W^T) = n$$
\end{theorem}
\begin{remark}
	برای هر تابع یک‌به‌یک خطی مانند
	$h$
	تابع وارون 
	$h^{-1}$
	نیز خطی است:
\end{remark}

برای بردار
$\mathcal{V} = (v_1, \ldots, v_m)$ 
و مجموعه
$S = \{s_1, \ldots, s_k\} \subseteq [m]$
بردار
$\mathcal{V} [S]$ 
با 
$k$ درایه 
را به این صورت تعریف می‌کنیم که شامل
$k$ 
درایه از
$V$ 
با اندیس‌های
$S$, 
است یعنی:\\
$\mathcal{V} [S] = (v_{s_1}, \ldots, v_{s_k})$.
همچنین تعریف می‌کنیم:
$\mathcal{V} [i] := \mathcal{V} [{\{i\}}]$

لم بعدی از مقاله
\cite{pliable2015paper}
را در فصل ۳ نیاز داریم.
\begin{lemma}
	\label{lemma:pliable20151}
	برای یک میدان به اندازه‌ی کافی بزرگ
	$\mathcal{F}$
	و
	$m$
	عضو آن،
	$n_0 \leq m$
	ترکیب خطی از این اعضا وجود دارد که برای هر زیرمجموعه‌ی
	$m_0 \leq n_0$
	عضوی از این اعضا را می‌توان با دانستن بقیه‌ی
	$m - m_0$
	عضو بازیابی کرد.
\end{lemma}
\begin{proof}
این نتیجه با استفاده از ماتریس‌های کوشی به عنوان ماتریس ضرایب ترکیب‌های خطی به دست می‌آید
\cite{Blmer1995AnXE}.
در واقع
$n_0$
ترکیب خطی به یک دستگاه معادلات خطی با
$m_0 \leq n_0$
مجهول در
$\mathcal{F}$
تبدیل می‌شوند. از آن‌جایی که هر زیرماتریس مربعی از یک ماتریس کوشی، ناتکین است، این دستگاه قابل حل است. برای این‌که ماتریس اصلی، کوشی شود تنها کافی است که اندازه‌ی میدان حداقل
$2m$
باشد. در نتیجه فرض می‌کنیم که اندازه‌ی میدان حداقل
$2m$
است
\cite{pliable2015paper}.
\end{proof}
	برای مطالعه‌ی بیشتر درباره جبرخطی به
\cite{sheldon}
مراجعه کنید.
\section{نظریه گراف}
\begin{definition}
یک گراف مانند
$G$
یک دوتایی مرتب
$(V, E)$
است به طوری که
$E = \{e: e = (x, y), x, y \in V$.
 $V$
را مجموعه‌ی رئوس و
$E$
را مجموعه‌ی 
\transf{یال‌}{edge}ها
می‌نامیم.

برای راس
$v\in V$
به مجمو‌عه‌ی تمام رئوسی مانند
  $u$ 
  که
  $\{u,v\}\in E$ 
  همسایه‌های 
  $v$
  می‌گوییم و با
  $N(v)$
  نمایش می‌دهیم.
  
به گراف
$G$
دوبخشی می‌گوییم اگر راس‌های آن را بتوان به دو مجموعه 
$A, B$
افراز کرد، طوری که دو سر هیچ یالی در یک مجموعه نباشد.
\end{definition}
	برای مطالعه‌ی بیشتر درباره‌ی نظریه گراف به
\cite{west}
مراجعه کنید.

