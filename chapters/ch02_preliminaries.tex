% !TeX spellcheck = fa-IR
\chapter{پیش‌نیازها}
\label{chapter:preliminaries}
در این فصل به مرور برخی از تعاریف و قضایای اولیه در شاخه‌های مختلف ریاضی که پیش‌نیاز مطالعه‌ی \picod هستند، می‌پردازیم. در هر بخش تنها مواردی که به طور قابل توجهی‌ مورد نیاز هستند آورده شده‌اند. در انتهای هر بخش مرجعی برای مبحث مورد نظر پیشنهاد شده است.
\section{احتمال}
\begin{definition}[\transf{متغیر برنولی}{bernoulli random variable}]
	به متغیر تصادفی گسسته
	$X$
	با مقادیر
	$0$
	و
	$1$
	که تابع توزیع آن برابر
	$P(X = 1) = p = 1 - P(X = 0)$
	باشد، متغیر برنولی گویند.
\end{definition}
\begin{definition}[\transf{متغیر مستقل با توزیع یکسان}{i.i.d, Independent and identically distributed}]
به مجموعه‌ای از متغیرهای تصادفی که از هم دیگر مستقل‌‌اند و تابع توزیع یکسانی دارند، گفته می‌شود.
\end{definition}
\begin{definition}[\transf{تابع محدب}{convex function}]
	تابع
	$f: X \rightarrow \mathbb{R}$
	با مقادیر حقیقی، یک تابع محدب نامیده می‌شود اگر هر پاره خط میان دو نقطه‌ی متمایز از آن در نمودار تابع بالای تابع قرار بگیرد. به زبان ریاضی:
	$$\forall t: 0 < t < 1, \forall x_1, x_2: x_1 \ne x_2: \Rightarrow f(x_1 + (1 - t) x_2) \leq tf(x_1) + (1 - t) f(x_2)$$
\end{definition}
\begin{theorem}[\transf{نامساوی ینسن}{Jensen's inequality}]
	\label{Jensen}
	برای تابع محدب
	$f$
	داریم:
	$$f(\mathbb{E}[X]) \leq \mathbb{E}[f(X)]$$
\end{theorem}
\begin{notation}[$\log^{+}$]
	$$\log^{+}(x) = \max\set{\log(x), 0}$$
\end{notation}
برای مطالعه‌ی بیشتر درباره‌ی احتمال به
\cite{book:pro}
مراجعه کنید.
\section{نظریه بازی‌ها}
موضوع مورد بحث در نظریه بازی‌ها، بررسی یک سیستم متشکل از تعدادی تصمیم‌گیر است. هر تصمیم‌گیر می‌تواند از بین تصمیمات قابل اتخاذ برای خود یک مورد را انتخاب کند. سپس بر اساس تصمیم تمام تصمیم‌گیران، سود یا ضرر هر تصمیم‌گیر مشخص می‌شود.

برای مثال فرض کنید دو نفر در حال انجام بازی سنگ کاغذ قیچی هستند. هر کدام می‌توانند یکی از سه حالت مختلف را انتخاب کنند و بر اساس انتخاب هر دو نفر یا یکی از آن‌ها پیروز شده و دیگری می‌بازد یا بازی به صورت برابر تمام می‌شود.
\begin{definition}[\transf{بهره‌وری پَرِتو}{Pareto efficiency}]
	\label{def:Pareto}
	به وضعیت سیستمی که تغییر انتخاب هیچ فردی نمی‌تواند بدون کاهش سود دیگران، سودش را افزایش دهد، گویند
	\cite{wiki:pareto}.
	\end{definition}
		\begin{definition}\transf{مرز پَرِتو}{Pareto boundary}
	\label{def:Pareto-boundary}
	به مجموعه نقاط فضای راه‌حل که در شرایط بهروه‌وری پرتو صدق می‌کنند گویند.
\end{definition}

	برای مطالعه‌ی بیشتر درباره‌ی نظریه‌ی بازی‌ها به
\cite{book:game}
مراجعه کنید.
\section{نظریه کدگذاری}
نظریه کدگذاری به بررسی و تحلیل کد‌ها می‌پردازد. در مخابرات و فناوری اطلاعات، یک کد، مجموعه‌‌ای از قوانین (به زبان ریاضی یک نگاشت‌) برای تبدیل داده‌ها از شکلی به شکل دیگر است، برای اهداف مختلف مانند کم‌ کردن میزان اطلاعات رد و بدل‌‌ شده برای رساندن پیام یا حفاظت از داده در برابر شنود و \dots .
از اولین نمونه‌ کدهای ساخته‌شده به دست انسان، می‌توان به زبان اشاره کرد که از مجموعه‌ای از اصوات و ترکیب آن‌ها برای انتقال پیام‌های مختلف استفاده می‌شود. به طور کلی چهار نوع کد وجود دارد \cite{wiki:coding1, wiki:coding2}:
\begin{enumerate}
	\item فشرده‌سازی منبع
	\item تصحیح خطا
	\item رمزگذاری
	\item کدگذاری خط
\end{enumerate}

در واقع موضوع این پایان‌نامه یعنی کدگذاری اندیس منعطف، خود نیز یک نوع کدگذاری فشرده‌سازی منبع است!

در این‌جا قصدی برای بررسی بیشتر نظریه کدگذاری نداریم و تنها به مرور موارد مورد نیاز خود می‌پردازیم.
\begin{example}
	فرض کنید آزاده و بردیا\footnote{طبق سنت دانشکده، به جای آلیس و باب از دو تا از دانشجویان قدیم دکتر محمودیان در دانشکده یعنی آزاده و بردیا به عنوان طرفین رمزنگاری استفاده می‌شود.}
	 می‌خواهد یک رشته هفت حرفی از اعداد باینری را برای بردیا ارسال کند ولی کانال ارتباطی آن‌ها دچار مشکل است و ممکن است در هربار ارسال با احتمال
	$p$
	بیت ارسالی را به اشتباه ارسال کند. بردیا چگونه می‌تواند مطمئن شود که دقیقا پیام آزاده را دریافت کرده است؟
	
	در این‌جا با کدهای مدیریت خطا سروکار داریم. در واقع می‌خواهیم کدی بسازیم که با استفاده از آن خطای ایجاد شده هنگام ارسال پیام را متوجه شویم و گاهی حتی توانایی بازیابی پیام اصلی را از روی پیام ارسال‌شده‌ی دارای نویز داشته باشیم. در مثال بالا اگر
	$p < 2^{-7}$
	باشد در این صورت اگر آزاده پس از ارسال هفت بیت پیام اصلی خود، باقی‌مانده‌ی جمع بیت‌های ارسالی بر دو را نیز برای بردیا ارسال کند. بردیا پس از دریافت اطلاعات، می‌تواند بررسی کند که آیا با احتمال بالایی پیام دریافتی همان پیام ارسالی هست یا نه. به مجموعه‌ پیام‌هایی که ممکن است در فرایند کدگذاری ارسال شوند 
	\transf{کدواژه}{code-word}
	گفته می‌شود.
	
	در مثال قبل از یک کد به شکل
	$C: \ff_2^7 \rightarrow \ff_2^8$
	استفاده کردیم. در این مثال
	$2^7$
	کدواژه مختلف داریم که طول هر یک
	$8$
	است.
\end{example}

\begin{definition}[\transf{فاصله‌ی همینگ}{hamming distance}
	 دو کدواژه]
	برای دو کدواژه‌ی مختلف
	$c_1, c_2 \in \ff_q^n$
	فاصله‌ی دو کدواژه
	$d(c_1, c_2)$
	را برابر تعداد درایه‌های متفاوت بردار
	$c_1$
	با بردار
	$c_2$
	تعریف می‌کنیم.
	
	 فاصله‌‌ی کمینه مجموعه‌ی
	$C$
	از کدواژه‌های با طول
	$n$
	 را برابر
	$d(C) = \min\limits_{x, y \in C, x \ne y} d(x, y)$
	تعریف می‌کنیم.
\end{definition}

\begin{definition}[\transf{	کران سینگلتون}{singleton bound}\cite{wiki:Singleton}]
	اگر
	$A_q(n, d)$
	را حداکثر تعداد کدواژه‌ها در یک کد
	$n$حرفی
 از الفبای
	$\ff_q$
	با فاصله کمینه‌ی 
	$d$
	در نظر بگیریم، داریم:
	\begin{equation}
		A_q(n, d) \leq q^{n - d + 1}
	\end{equation}
\end{definition}
\begin{definition}[\transf{کد ام‌دی‌اس}{MDS code}]
	\label{def:mds}
	به کدهایی که کران سینگلتون را کسب کنند گفته می‌شود.
\end{definition}

کدهای ام‌دی‌اس بیشترین فاصله‌ی کمینه را دارند و در نتیجه بهترین کدها به عنوان کدهای مدیریت خطا هستند.
\begin{example}
	$n$
	عضو ناصفر
	$\alpha_1, \ldots, \alpha_n \in \ff_q$
	در نظر بگیرید. کد تولید شده توسط ماتریس 
	$H = (h_{ij}): h_{ij} = \alpha_j^i: i = [n - k], j = [n]$
	(که به
	\transf{ماتریس بررسی زوجیت}{parity-check matrix}
	 معروف است.) یک کد ام‌دی‌اس است.
\end{example}

	برای مطالعه‌ی بیشتر درباره‌ی نظریه کدگذاری به
\cite{book:coding}
مراجعه کنید.
\section{نظریه اطلاعات}
تابع آنتروپی برای نشان دادن میزان ابهام یک متغیر تصادفی تعریف می‌شود. اگر متغیر تصادفی
$X$
از مجموعه‌ی مرجع
$[n]$
به صورت یک‌نواخت انتخاب شود، تعریف می‌کنیم:
$$H(X) = \log(n)$$
در واقع آنتروپی در این حالت نشان‌دهنده‌ی تعداد بیت لازم برای این است که به فرد دیگری اطلاع دهیم مقدار مشاهده شده در نمونه‌گیری انجام شده از متغیر تصادفی چیست. 


\begin{definition}[آنتروپی]
برای متغیر تصادفی
$X$
با مجموعه‌ی مرجع
$[n]$
که عدد
$i$
را با احتمال
$p_i$
می‌گیرد میزان اطلاعات یا آنتروپی را به این صورت تعریف می‌کنیم:
$$H(X) = \sum_{i = 1}^{n} p_i \log(\frac{1}{n})$$
\end{definition}
\begin{theorem}
بیشینه آنتروپی برای متغیر تصادفی که
$n$
مقدار مختلف را به خود می‌گیرد، زمانی رخ می‌دهد که
$$p_1 = p_2 = \dots p_n = \frac{1}{n}$$
\end{theorem}
\begin{proof}
برای این اثبات به نامساوی ینسن(\autoref{Jensen})
 نیاز داریم. برای تابع محدب
$f$
و نقاط
$a_1, a_2, \dots, a_n$
از دامنه‌ی آن و
$p_i \in [0, 1]$
به طوری که
$\sum_{i = 1}^{n} p_i = 1$
داریم:
$$f(\sum_{i = 1}^{n} p_i a_i) \leq \sum_{i = 1}^{n} p_i f(a_i)$$
حال اگر به جای
$f$
تابع محدب
$-\log$
را قرار دهیم اثبات کامل می‌شود.
\end{proof}

\subsection{گزاره‌های ابتدایی}
\begin{theorem}
اگر
$X$
و
$Y$
دو متغیر تصادفی مستقل باشند و
$W = (X, X)$
و
$Z = (X, Y)$
داریم
\begin{align*}
    H(W) &= H(X) \\
    H(Z) &= H(X) + H(Y)
\end{align*}
\end{theorem}
\begin{definition}[آنتروپی شرطی]
	به میزان ابهام یک متغیر در صورت دانستن مقدار یک متغیر دیگر گفته می‌شود و به صورت زیر تعریف می‌شود:
$$H(X | Y = y) := \sum_{X = x} P(X = x| Y = y) \log(\frac{1}{P(X = x| Y = y)})$$
$$H(X|Y) := \sum_{Y = y} P(Y = y) H(X | Y = y)$$
\end{definition}

\begin{theorem}
اگر
$X$
و
$Y$
مستقل باشند:
$$H(X|Y) = H(X)$$
\end{theorem}
\begin{theorem}
	نامساوی‌های زیادی را می‌توان تنها با استفاده از تعریف آنتروپی به دست آورد در اینجا به چهار نامساوی اولیه که کاربردی‌ترند اشاره می‌کنیم:
	\begin{enumerate}
\item $H(X) \geq 0 $
\item $Z = (X, Y) \rightarrow H(X, Y) := H(Z) \rightarrow H(X, Y) = H(Y) + H(X|Y) $
\item $H(X_1, X_2, \dots X_n) = H(X_1) + H(X_2 | X_1) + \dots + H(X_n | X_1 X_2 \dots X_n)$
\item $H(X_1, X_2, \dots X_n) \leq \sum_{i = 1}^{n} H(X_i)$
\end{enumerate}
\end{theorem}
\subsection{اطلاعات متقابل}
\begin{definition}
آنتروپی نسبی یا فاصله کولبک لیبلر: برای دو تابع احتمال به شکل زیر تعریف می‌شود:
$$
D(p||q) = \sum_{x \in \mathcal{X}} p(x) \log(\dfrac{p(x)}{q(x)})
$$
که برابر است با:
$$E_p(x) \log(\dfrac{p(X)}{q(X)})$$
\end{definition}
\begin{theorem}
آنتروپی نسبی همیشه نامنفی است و برابر صفر است اگر و تنها اگر
$p = q$
\end{theorem}
این تابع واقعا یک تابع فاصله متریک ریاضیاتی نیست ولی تفاوت بین دو توزیع را شبیه سازی می‌کند.
\begin{definition}[اطلاعات متفابل]
$$I(X; Y) := \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log(\dfrac{p(x, y)}{p(x)p(y)})$$
که برابر است با:
\begin{align*}
    I(X;Y) &= D(p(x, y) || p(x)p(y))
    \\
    &= E_{p(x, y)} \log(\dfrac{p(X, Y)}{p(X)p(Y)})
\end{align*}
\end{definition}

فرض کنید با دانستن مقدار یک نمونه از متغیر تصادفی
$Y$
مقدار نمونه متغیر تصادفی
$X$
که با
$Y$
همبسته است را حدس بزنیم. میدانیم که
$H(X|Y) = 0$
اگر و تنها اگر
$X$
تابعی از
$Y$
باشد. در قضیه‌ی زیر این مسئله به طور دقیق‌تر بیان شده است.
\begin{theorem}[نامساوی فانو]
فرض کنید
$X$ و $Y$
دو متغیر تصادفی از مجموعه‌ی مرجع
$\mathcal{X}$
باشند که
$P(x, y)$
تابع احتمال توأم آن‌ها است و همچنین داریم
$Y = f(X)$.
اگر تعریف کنیم
$P_e := P(X \neq Y)$
و همچنین
$H_b(P_e) = - P_e \log P_e - (1 - P_e) \log(1 - P_e)$،
آنگاه خواهیم داشت:
$$H(X|Y) \leq H_b(P_e) + P_e \log(\mathcal{X} - 1)$$
\end{theorem}
\begin{remark}
	فرض کنید یک کانال ارتباطی دارای نویز داریم که از آن برای ارسال مقدار
	$X$
	استفاده می‌کنیم. اگر مقدار دریافت شده را
	$Y$
	بگیریم خواهیم داشت.
	$Y = f(X)$
	 نامساوی فانو یک حد پایین ارائه می‌دهد که برای یک
$f$
داده شده (که
$f$
را در حالت کلی می‌توان به چشم یک تخمین از
$X$
یا یک مشاهده نویزدار از
$X$
نگاه کرد.) هیچ تابع
$G:Y \rightarrow X$
(یک تخمین برای مقدار
$X$)
وجود ندارد که بهتر از این حد عمل کند و به بیان دیگر همیشه حداقل این مقدار خطا را خواهیم داشت.
\end{remark}

	برای مطالعه‌ی بیشتر درباره‌ی نظریه اطلاعات به
\cite{book:info}
مراجعه کنید.
\section{جبرخطی}
\begin{definition}[پوشش خطی]
	برای مجموعه بردار‌های
	$S$، \transf{پوشش خطی}{span}،
	ترکیب خطی اعضای آن است یعنی:
	$\sspan(S) = \set{ \sum\limits_{v \in S} \lambda v}$
	
	برای ماتریس
	$M$، $\sspan(M)$
را پوشش خطی سطرهای 
$M$
می‌گیریم.
\end{definition}

\begin{definition}[هم‌دسته]
	\label{coset}
	برای یک زیرفضای برداری
	$V \subseteq \mathbb{F}^l$
	به
	$W$
	هم‌دسته‌‌‌ی
	$V$
	می‌گوییم اگر یک بردار
	$x \in \mathbb{F}^l$
	وجود داشته باشد که
	$$W = V + x = \set{w: w \in \mathbb{F}^l, \exists v \in V, w = v + x}$$
	به هم‌دسته‌های یک زیرفضای برداری،
	\transf{زیرفضاهای آفین}{Affine subspaces}
	گفته می‌شود.
\end{definition}
\begin{definition}[\transf{هسته}{kernel}]
	هسته ماتریس
	$M$
	بصورت زیر تعریف می‌شود:
	$$\kerr(M) = \set{v: Mv = 0}$$
\end{definition}
\begin{definition}[فضای عمود]
برای فضای برداری
$W$،
فضای عمود یا \transf{مکمل عمود}{orthogonal complement}
 را بصورت
$W^\bot = \set{v: \forall w \in W, v^T \times w = 0}$
تعریف می‌کنیم.
\end{definition}

\begin{theorem}
	برای هر فضای برداری متناهی داریم:
	$$\dim(W) + \dim(W^T) = n$$
\end{theorem}
\begin{remark}
	برای هر تابع یک‌به‌یک خطی مانند
	$h$
	تابع وارون 
	$h^{-1}$
	نیز خطی است.
\end{remark}
\begin{notation}
برای بردار
$\mathcal{V} = (v_1, \ldots, v_m)$ 
و مجموعه
$S = \set{s_1, \ldots, s_k} \subseteq [m]$
بردار
$\mathcal{V} [S]$ 
با 
$k$ درایه 
را به این صورت تعریف می‌کنیم که شامل
$k$ 
درایه از
$V$ 
با اندیس‌های
$S$, 
است یعنی:\\
$\mathcal{V} [S] = (v_{s_1}, \ldots, v_{s_k})$.
همچنین تعریف می‌کنیم:
$\mathcal{V} [i] := \mathcal{V} [{\set{i}}]$
\end{notation}

لم بعدی از مقاله
\cite{pliable2015paper}
را در فصل ۳ نیاز داریم.
\begin{lemma}
	\label{lemma:pliable20151}
	برای یک میدان به اندازه‌ی کافی بزرگ
	$\ff$
	و
	$m$
	عضو آن،
	$n_0 \leq m$
	ترکیب خطی از این اعضا وجود دارد که برای هر زیرمجموعه‌ی
	$m_0 \leq n_0$
	عضوی از این اعضا را می‌توان با دانستن بقیه‌ی
	$m - m_0$
	عضو بازیابی کرد.
\end{lemma}
\begin{proof}
این نتیجه با استفاده از ماتریس‌های کوشی به عنوان ماتریس ضرایب ترکیب‌های خطی به دست می‌آید
\cite{Blmer1995AnXE}.
در واقع
$n_0$
ترکیب خطی به یک دستگاه معادلات خطی با
$m_0 \leq n_0$
مجهول در
$\ff$
تبدیل می‌شوند. از آن‌جایی که هر زیرماتریس مربعی از یک ماتریس کوشی، 
\transf{
ناتکین 
}{non singular}
است، این دستگاه قابل حل است. برای این‌که ماتریس اصلی، کوشی شود تنها کافی است که اندازه‌ی میدان حداقل
$2m$
باشد. در نتیجه فرض می‌کنیم که اندازه‌ی میدان حداقل
$2m$
است
\cite{pliable2015paper}.
\end{proof}
	برای مطالعه‌ی بیشتر درباره جبرخطی به
\cite{sheldon}
مراجعه کنید.
\section{نظریه گراف}
\begin{definition}[گراف]
یک گراف مانند
$G$
یک دوتایی مرتب
$(V, E)$
است که:
$$E = \set{e: e = (x, y), x, y \in V}$$
 $V$
را مجموعه‌ی رئوس و
$E$
را مجموعه‌ی 
\transf{‌یال‌ها‌}{edge}
می‌نامیم.
\end{definition}
\begin{definition}[همسایه]
برای رأس
$v\in V$
به مجمو‌عه‌ی تمام رأس‌های مانند
  $u$ 
  که
  $\set{u,v}\in E$
  همسایه‌های 
  $v$
  می‌گوییم و با
  $N(v)$
  نمایش می‌دهیم.
  \end{definition}
  \begin{definition}[گراف دوبخشی]
  
به گراف
$G$
دوبخشی می‌گوییم اگر رأس‌های آن را بتوان به دو مجموعه
$A, B$
افراز کرد، طوری که دو سر هیچ یالی در یک مجموعه نباشد.
\end{definition}

\begin{theorem}
	\label{thm:vertexx}
	برای گراف راس-ترایا
	$G$
	داریم:
	$$\dfrac{n}{\alpha(G)} \leq \chi(G) \leq \dfrac{n(1 + \ln n)}{\alpha(G)}$$
\end{theorem}
	برای اثبات این قضیه به
	\lr{Proposition 3.12}
	در فصل 27 کتاب
	\cite{graham1995handbook}
	رجوع کنید.
	
	برای مطالعه‌ی بیشتر درباره‌ی نظریه گراف به
\cite{west}
مراجعه کنید.

