\chapter{پیش‌نیازها}
\label{chapter:preliminaries}
\section{مفاهیم اولیه}

در این فصل به مرور برخی ای تعاریف و قضایای اولیه در شاخه‌های مختلف ریاضی که پیش‌نیاز مطالعه‌ی کدگذاری اندیس است می‌پردازیم.


\begin{definition}[
	کد
	\lr{mds}
	]
	\label{def:mds}
\end{definition}

\begin{definition}[
	\lr{ Pareto trade-off}
	]
	\label{def:Pareto}
    به فلان میگویند.

    مرز پارتو
    \lr{Pareto boundary}
    \label{def:Pareto-boundary}
\end{definition}

\section{
    ‌نظریه اطلاعات
}

تابع آنتروپی را برای نشان دادن میزان ابهام یک متغیر تصادفی تعریف میکنیم. اگر متغیر تصادفی
$x$
از مجموعه‌ی
$[n]$
به صورت یکنواخت بیایید تعریف میکنیم:
$$H(x) = \log(n)$$
در واقع آنتروپی در این حالت نشان دهنده‌ی تعداد بیت لازم برای این است که به فرد دیگری اطلاع دهیم سمپل متغیر تصادفی کدام مقدار بوده است. در واقع بعد از سمپل گرفتن از متغیر تصادفی سوال شما این خواهد بود که کدام عضو مجموعه‌ی ساپورت آمده است و برای پاسخ به این سوال من باید
$\log(n)$
به شما داده بدهم.
برای متغیر تصادفی
$X$
با محموعه‌ی ساپورت
$[n]$
که عدد
$i$
را با احتمال
$p_i$
میگیرد میزان اطلاعات یا آنتروپی را به این صورت تعریف میکنیم:
$$H(X) = \sum_{i = 1}^{n} p_i \log(\frac{1}{n})$$

\begin{theorem}
بیشینه آنتروپی برای متغیر تصادفی با
$n$
مقدار زمانی رخ میدهد که
$$p_1 = p_2 = \dots p_n = \frac{1}{n}$$
\end{theorem}
\begin{proof}
برای این اثبات به نامساوی سنسن نیاز داریم: برای تابع محدب
$f$
و نقاط
$a_1, a_2, \dots, a_n$
از دامنه آن و
$p_i \in [0, 1]$
به طوری که
$\sum_{i = 1}^{n} p_i = 1$
داریم:
$$f(\sum_{i = 1}^{n} p_i a_i) \leq \sum_{i = 1}^{n} p_i f(a_i)$$
اگر تابع
$- log$
را که محدب است در بالا قرار دهیم مسئله حل میشود
\end{proof}

\subsection{گزاره‌های ابتدایی}
\begin{theorem}
اگر
$X$
و
$Y$
دو متغیر تصادفی مستقل باشند و
$W = (X, X)$
و
$Z = (X, Y)$
داریم
\begin{align*}
    H(W) &= H(X) \\
    H(Z) &= H(X) + H(Y)
\end{align*}
\end{theorem}
\begin{proof}
	با توجه به تمرین خواهیم داشت:
$$H(Z) = \sum_{X = x, Y = y} P(X = x, Y = y) \log(\frac{1}{ P(X = x, Y = y)})$$
بقیه اثبات سر راست است.
\end{proof}
\begin{definition}
$$H(X | Y = y) := \sum_{X = x} P(X = x| Y = y) \log(\frac{1}{P(X = x| Y = y)})$$
$$H(X|Y) := \sum_{Y = y} P(Y = y) H(X | Y = y)$$
\end{definition}

\begin{theorem}
اگر
$X$
و
$Y$
مستقل باشند:
$$H(X|Y) = H(X)$$
\end{theorem}
\begin{theorem}
$$H(X) \geq 0$$
$$Z = (X, Y) \rightarrow H(X, Y) := H(Z) \rightarrow H(X, Y) = H(Y) + H(X|Y) $$
$$H(X_1, X_2, \dots X_n) = H(X_1) + H(X_2 | X_1) + \dots + H(X_n | X_1 X_2 \dots X_n)$$
$$H(X_1, X_2, \dots X_n) \leq \sum_{i = 1}^{n} H(X_i)$$
\end{theorem}
\subsection{اطلاعات متقابل}
\begin{definition}
آنتروپی نسبی/فاصله کولبک لیبلر: برای دو تابع احتمال به شکل زیر تعریف میشود:
$$
D(p||q) = \sum_{x \in \mathcal{X}} p(x) \log(\dfrac{p(x)}{q(x)})
$$
که برابر است با:
$$E_p \log(\dfrac{p(X)}{q(X)})$$
\end{definition}
\begin{theorem}
آنتروپی نسبی همیشه نامنفی است و برابر صفر است اگر و تنها اگر
$p = q$
\end{theorem}
این تابع واقعا تابع فاصله ریاضیاتی نیست ولی تفاوت بین دو توزیع را شبیه سازی می‌کند.

\begin{definition}
اطلاعات متفابل:
$$I(X; Y) := \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log(\dfrac{p(x, y)}{p(x)p(y)})$$
که برابر است با:
\begin{align*}
    I(X;Y) &= D(p(x, y) || p(x)p(y))
    \\
    &= E_{p(x, y)} \log(\dfrac{p(X, Y)}{p(X)p(Y)})
\end{align*}
\end{definition}
نامساوی فانو: فرض کنید با دانستن مقدار یک سمپل از متغیر تصادفی
$Y$
مقدار سمپل متغیر تصادفی
$X$
که با
$Y$
همبسته است را حدس بزنیم. میدانیم که
$H(X|Y) = 0$
اگر و تنها اگر
$X$
تابعی از
$Y$
باشد. 
\begin{theorem}[
	فانو
	]
فرض کنید
$P(x, y)$
تابع احتمال توام دو متغیر تصادفی باشد که
$X$
از مجموعه
$\mathcal{X}$
می‌آیند و
$Y = f(X)$
تعریف میکنیم:
$$P_e := P(X \neq \hat{X})$$
خواهیم داشت:
$$H(X|Y) \leq H_b(P_e) + P_e \log(\mathcal{X} - 1)$$
\end{theorem}
\begin{remark}
نامساوی فانو برای ما یک حد پایین میاید که برای یک
$f$
داده شده(یک تخمین از
$X$
 یک مشاهده نویزدار از
$X$
) هیچ تابع
$G:\mathcal{Y} \rightarrow \mathcal{X}$
ای(یک تخمین برای مقدار
$X$
) وجود ندارد که بهتر از این حد عمل کند و به بیان دیگر همیشه حداقل این مقدار خطا خواهیم داشت.
\end{remark}
\section{جبرخطی}
\begin{definition}
	برای مجموعه‌ی بردار‌های
	$S$
		\transf{
		پوشش خطی
	}{span}
	ترکیب خطی اعضای آن است یعنی:
	$span(S) = \{ \sum\limits_{v \in S} \lambda v\}$
	
	برای ماتریس
	$M$
آن را پوشش خطی سطرهای 
$M$
می‌گیریم.

\begin{definition}[
	همدسته
	]
	برای یک زیر فضای برداری
	$V \subseteq \mathbb{F}^l$
	به
	$W$
	هم‌دسته‌‌‌ی
	$V$
	می‌گوییم اگر یک بردار
	$x \in \mathbb{F}^l$
	وجود داشته باشد که
	$$W = V + x = \{w: w \in \mathbb{F}^l, \exists v \in V, w = v + x\}$$
	هم‌دسته‌های یک زیرفضای برداری،
	\transf{
	زیرفضاهای آفین
	}{
	Affine subspaces
	}
	گفته می‌شود.
\end{definition}
\begin{definition}[
	هسته
	]
	\transf{
	هسته
	}{kernel}
	ماتریس
	$M$
	به صورت زیر تعریف می‌شود:
	$$ker(M) = \{v: Mv = 0\}$$
\end{definition}
برای فضای برداری
$W$
فضای عمود یا
\transf{
مکمل عمود
}{orthogonal complement}
 را به صورت
$W^\bot = \{v: \forall w \in W, v^T.w = 0\}$
تعریف می‌کنیم.

\begin{theorem}
	برای هر فضای برداری متناهی داریم:
	$$dim(W) + dim(W^T) = n$$
\end{theorem}
\begin{remark}
	برای هر تابع یک‌به‌یک خطی مانند
	$h$
	تابع وارون 
	$h^{-1}$
	نیز خطی است:
\end{remark}

برای بردار
$\mathcal{V} = (v_1, \ldots, v_m)$ 
و مجموعه
$S = \{s_1, \ldots, s_k\} \subseteq [m]$
بردار
$\mathcal{V} [S]$ 
با 
$k$
درایه را به این صورت تعریف میکنیم که شامل 
$k$ 
درایه از
$V$ 
با اندیس های
$S$, 
است یعنی:
$\mathcal{V} [S] = (v_{s_1}, \ldots, v_{s_k})$
همچنین تعریف میکنیم:
$\mathcal{V} [i] := \mathcal{V} [{\{i\}}]$

\section{نظریه گراف}
\begin{definition}
یک گراف مانند
$G$
یک دو تایی مرتب
$(V, E)$
است به طوری که
$E = \{e: e = (x, y), x, y \in V$.
 $V$
را مجموعه‌ی رئوس و
$E$
را مجموعه‌ی یال‌ها می نامیم.

برای راس
$v\in V$
به مجمو‌عه‌ی تمام رئوسی مانند
  $u$ 
  که
  $\{u,v\}\in E$ 
  همسایه‌های 
  $v$
  می‌گوییم.
  و با
  $N(v)$
  نشانشان می‌دهیم.
  
به گراف
$G$
دو بخشی می‌گوییم اگر راس‌های آن را بتوان به دو مجموعه 
$A, B$
افراز کرد دو سر هیچ یالی در یک مجموعه نباشد.
\end{definition}

