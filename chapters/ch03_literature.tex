\chapter{کارهای پیشین}
\label{chapter:literature}

در این فصل به بررسی پژوهش‌های گذشته بر روی مسئله‌ی کدگذاری اندیس منعطف می‌پردازیم.

\section{مقدمه}
همان‌طور که در فصل اول گفته شد، مسئله‌ی کدگذاری اندیس منعطف توسط براهما و فرگولی در سال 2015 در
\cite{pliablefirstpaper}
به عنوان توسعه‌ای از مسئله‌ی کدگذاری اندیس معرفی شد. در این حالت هر گیرنده به جای اینکه به دنبال یک پیام خاص باشد، به دنبال یک پیام جدید است که در مجموعه‌ی اطلاعات جانبی‌ش نباشد.

این مسئله کاربردهایی مختلفی در مسائل حوزه‌های گوناگون دارد. برای مثال در محاسبات توزیع شده
\cite{datashuf}
\transf{
سیستم‌های پیشنهاد دهنده
}{recommendation system}
\cite{8404065}
\transf{
یادگیری توزیع شده
}{
distributed learning
}
\cite{8682270}.
در این فصل به مرور ادبیات پژوهشی 
\picod
می‌پردازیم.

در 
\autoref{sec:3:2}
ضمن دسته بندی مقالات این حوزه مروری اجمالی بر آن‌ها میکنیم.

در 
\autoref{sec:3:3}
به بررسی الگوریتمی تقریبی برای 
\picod
که توسط فرگولی و همکاران در
\cite{song2017polynomialtime}
معرفی شده است می‌پردازیم.
\section{
	مروری اجمالی بر مقالات حوزه
\picod
}
\label{sec:3:2}
سپس در فصل چهار با چهار گونه‌ی مختلف تعریف شده بر اساس کدگذاری اندیس منعطف آشنا خواهیم شد.
در فصل پنج به ارتباط
پس از آن مقالات متعددی روی این مسئله تمرکز کرده اند. پژوهش بر روی مسئله‌ی کدگذاری اندیس منعطف را می‌توان به دسته‌های زیر تقسیم کرد:
\begin{enumerate}
\item 
ساخت یک کد برای دسته‌ای از گراف‌ها و یا ساخت الگوریتم‌های تقریبی/تصادفی برای همه‌ی گراف‌ها
\item 
تعریف مسائل جدید بر پایه‌ی کدگذاری اندیس منعطف(مانند کدگذاری اندیس ترجیحی و ...)
\item 
استفاده از مسئله‌ی کدگذاری اندیس منعطف برای مدل سازی و حل مسائل دیگر
\item 
کران بالا برای حداکثر تعداد ارسال مورد نیاز و کران پایین برای حداقل تعداد ارسال مورد نیاز
\end{enumerate}

روی هر کدام از موضاع بالا مقالات متعددی کار کرده اند. برای مثال
\begin{enumerate}
	\item الگوریتم‌/کد جدید
	\begin{enumerate}
		\item 
		لیو در
		\cite{8278015}
		با استفاده از روش‌های نظریه اطلاعات به اثبات کران‌هایی برای چند خانواده مختلف از گراف‌های اطلاعات جانبی می‌پردازد.
		\item 
	در
	\cite{10313405}
	الگوریتمی برای.
	\item
	در
	\cite{8871209}
	مسئله را فقط برای بخشی از گراف‌های اطلاعات جانبی که
	\lr{complete–S PICOD}
	نام گذاری می‌کنند حل میکنند.
	\item
	در
	\cite{9759449}
	با تعریف یک مسئله‌ی
	\transf{
	بهینه سازی تنک و با رنک پایین}{
	sparse and low-rank optimization
	}
	و حل آن با 
	\transf{
	الگوریتم تصویر کردن تکراری
	}{Alternating Projection Algorithm}
	الگوریتمی کارا برای حل مسئله ارائه میدهند.
	\item
	در
	\cite{8682270}
	با استفاده از
	\picod
	و الگوریتم بر مبنای
	 \transf{
	 تفاوت تحدب
	 }{
	 	difference-of-convex
	 }
	 ارائه می‌دهند که بر اساس نتایج آزمایشگاهی باعث کاهش پهنای باند مورد نیاز در
	 \transf{
	 یادگیری توزیع شده روی دستگاه‌های نهایی
	 }{ON-DEVICE DISTRIBUTED LEARNING}
	 می‌شود.
	 \item
	 در
	 \cite{sasi2019pliable}
	 برای کلاس خاصی از مسئله‌ی
	 \picod
	 که
	 \transf{
	 متوالی
	 }{consecutive}
	 نامیده می‌شود بحث می‌کنند و برای دو حالت اکستریم آن کد اندیس ارائه می‌دهند. سپس در ادامه برای حالت
	 \lr{c-Constrained}
	 نیز کد ارائه می‌دهند.
	 \item 
	 در
	 \cite{8613483}
	 شبیه مقاله قبلی بر روی
	 \lr{Consecutive Complete–S}
	 کار می کنند و با استفاده از اثبات‌های ترکیبیاتی، اثباتی برای وجود کد با طول مناسب ارائه می‌دهند.
	\end{enumerate}
	 
	\item گونه‌های جدید
	\begin{enumerate}
		\item 
		لیو و همکاران در
		\cite{10015670}
		به مسئله‌ی نشتی ناخواسته‌ی اطلاعات در 
		\icod
		و
		\picod
		می‌پردازند. اگر بخشی از پیام‌ها حساس و بقیه غیر حساس باشند یک شنودکننده‌ی متخاصم بر اساس پیام‌های دریافتی چه مقدار داده کسب خواهد کرد؟ این مقدار را با
		\transf{
		نرخ نشت
		}{leakage rate}
		نشان می‌دهیم. در ادامه نرخ نشت بهینه برای مسائل
		\icod
		را اثبات می‌کنند و الگوریتم قطعی برای پیدا کردن آن ارائه می‌دهند و نشان می‌دهند نتیجه‌ی به دست آمده برای
		\icod
		برای
		\picod
		هم برقرار است.
		\item 
		فرگولی در
		\cite{6620405}
		مسئله‌ی
		\picod
		را به دو روش تعمیم می‌دهد. در روش اول هر گیرنده به جای اینکه به دنبال بازیابی یک پیام باشد به دنبال بازیابی
		$t$
		پیام است که به آن
		\lr{MULT-PICOD}
		می‌گوید.
		در روش دوم فرستنده گراف اطلاعات جالبی را در دست ندارد و تنها می‌داند که گیرنده‌ها چند پیام را از پیش به عنوان اطلاعات جانبی دارند(همه‌ی گیرنده‌ها به تعداد برابر پیام دارند.) و آن را
		\lr{OB-PICOD}
		می‌نامد.
		\item 
		لینکی کار فرگولی در مقاله قبلی را در
		\cite{8625330}
		ادامه می‌دهد و روش کدگذاری جدید برای 
		\lr{MULT-PICOD}
		ارائه می‌دهد.
		\item 
		لیو در
		\cite{9173957}
		گونه جدیدی از مسئله را تعریف میکنند که اولا به جای وجود سرور مرکزی تبادل پیام به صورت نامتمرکز انجام می‌شود  و همچنین هر گیرنده تنها یک پیام جدید خارج از اطلاعات جانبی خود بازیابی می‌کند و هیچ دیتایی راجع به بقیه پیام‌ها کسب نمی‌کند. به دلیل سختی این مسئله در ادامه تنها روی یک حالت خاص که اطلاعات جانبی گیرنده‌های به صورت
		\transf{
		جابه‌جایی های چرخشی 
		$s$
		تایی
		}{$s$ circular shifts}
		هست تمرکز می‌کنند.	

	\end{enumerate}
	\item حل مسائل دیگر
		\begin{enumerate}
			\item 
	در
	\cite{Obead_2023}
	با استفاده از  
	\picod
	مسئله‌ی
	\transf{
	بازیابی منعطف و محرمانه دیتا، همراه با اطلاعات جانبی با یک سرور
	}{
	Single-Server Pliable Private Information Retrieval With Side Information
	}
	را حل میکنند.
			\item
	سانگ و فرگولی در
	\cite{8404065}
	به ارتباط 
	\picod
	و ساختن  سیستم‌های پیشنهاد دهنده‌ با توجه به پهنای باند می‌پردازند.
	\item
	در
	\cite{e24081149}
	به ارتباط 
	\icod
	و
	\picod
	با
	\lr{error-correcting codes with multiple interpretations from the
		tree construction of nested cyclic codes}
		می پردازند.
		\item 
		فرگولی در
		\cite{datashuf}
		با استفاده از
		\picod
		به مسئله‌ی
		\transf{
			بازآرایی داده
		}{Data Shuffling}
		که در مسائل محاسبه‌ی توزیع شده ضاهر می‌شود می‌پردازد.
	\item
	\namef{لینکی}{Song, Linqi}
	و فرگولی در
	
	\cite{7176784}
	بررسی‌ای اجمالی بر تاثیر ایده‌ی فکری پشت
	\picod
	بر دسته‌ای از مسائل مخابراتی که
	\transf{
	کدگذاری نوع محتوا
	}{Content-Type Coding}
	نامیده می‌شود می پردازند. لینکی در ادامه در پایان نامه‌ی دکتری خود
	\cite{linqiphd}
	نتایج متعددی در این زمینه می‌گیرد.
		\end{enumerate}
		\item کران بالا و پایین
		\begin{enumerate}
			\item 
			کران بالای
			$\mathcal{O}(\log(n))$
			برای
			\picod
			با کران بالای
			\icod
			که
			$\mathcal{O}(n)$
			است تفاوت فاحشی دارد. لیو و تانینتی در
			\cite{7606849}
		تلاش می‌کنند با پیدا کردن کرانی برای تعداد گیرنده‌هایی که با هر پیام می‌توان ارضا کرد شهودی برای این مسئله بیابند.
			\item 
			در
			\cite{9518120}
			گراف اطلاعات جانبی را با استفاده از 
			\transf{
			هایپرگراف‌ها
			}{hyper graphs}
			مدل‌سازی کرده و کران بالایی برای تعداد ارسال‌ها پیدا میکنند. سپس با استفاده از این کران اثبات می‌کنند که برای بعضی از حالت ها کد با طول خوبی وجود دارد.
			\item
			در ادامه مقاله قبل با روش مشابه‌ای در
			\cite{9965883}
			کران بالا و کران پایین برای
			\picod
			اثبات شده است.
					\item
					\transf{انگ}{ong}
					همکاران در
			\cite{ong2019improved}
			و سپس در
			\cite{8849527}
			تکنیک بسیار متفاوتی در پیش می‌گیرند. به جای بررسی اطلاعات جانبی گیرنده‌ها به 
			\transf{
			گیرنده‌های غایب
		}{Absent Receivers}
		می‌پردازند. اگر هر گیرنده را بر اساس اطلاعات جانبیش شناسایی کنیم، به زیرمجموعه‌های مجموعه‌ی پیام‌ها که به عنوان گیرنده در گراف وجود ندارند گیرنده غایب می‌گوییم. با استفاده از این روش کران پایین جدیدی برای حداقل ارسال مورد نیاز پیدا می‌کنند.
		\end{enumerate}
\end{enumerate}


\section{
\picod
}
\subsection{تعریف دقیق مسئله}
\label{sec:3:3}
fix this
در بخش به بررسی مقالات
\cite{pliablefirstpaper,6620405, pliable2015paper,  song2017polynomialtime}
(نسخه ژرنالی مقاله دوم
\cite{7541273}
) از فرگولی و همکاران می‌پردازیم. 

	سیستمی با یک فرستنده و 
	$n$
	گیرنده در نظر بگیرید. فرستنده مقدار
	$m$
	متغیر تصادفی
	$X = (X_1, \ldots, X_n)$
	که از میدان متناهی
	$\mathcal{F}$
	را در دست دارد. هر گیرنده مانند 
	$i$
	بخشی از متغیرهای تصادفی را در اختیار دارد که اندیس آن‌ها را با 
	$S_i \subseteq [m]$
	نشان می‌دهیم. هر گیرنده می‌خواهد 
	$t$
	پیام که جز اطلاعات جانبی‌اش نیست یعنی اندیسشان در
	$R_i = [m] \setminus S_i$
	باشد را بازیابی کند.
	فرستنده می‌تواند تعدادی پیام مانند
	$Y = (Y_1, \ldots, Y_l): Y_i \in \fq$
	را برای تمام گیرنده‌ها ارسال کند تا گیرنده‌ها بتوانند با پیام‌های ارسالی و اطلاعات جانبی خود پیام‌های مورد نظر را بازیابی کنند. در
	\picod
	قبل از ارسال پیام‌ها توسط فرستنده، پیام‌هایی که توسط هر گیرنده بازیابی خواهد کرد یعنی
	$U_i \subseteq R_i,  |U_i| = t$
	 مشخص می‌شود. برعکس 
	\icod
	 که هر گیرنده پیامی مشخص را باید بازیابی کند در این‌جا مشخص کردن پیام‌هایی که بازیابی می‌شوند نیز بخشی از مسئله است.
	 
	 مسئله 
	 \picodt
	 پیدا کردن الگوریتمی است که با گرفتن 
	 $S_i$
	 ها
	 $U$
	 و
	 $Y$
	 را پیدا کند به طوری که هر گیرنده بتواند
	 $t$
	 پیام جدید را بازیابی کند و 
	 $l = |Y|$
	 کمینه شود. مشابه
	 \autoref{def:icod}
	 به تعریف رسمی
	 \picodt
	 میپردازیم:
\begin{definition}[\picodt]
	\label{def:picodt}
	برای
	$n$
	و
	$m$
	و
	$S_i$
	های داده شده یک
	\picodt
	عبارت است از:
	\begin{enumerate}
		\item 
		تابع کدگذاری
		$E: \mathcal{F}^n \rightarrow \mathcal{F}^l$
		که به 
		$l$
		طول کد میگوییم.
		\item 
		توابع کدگشایی 
		$D_i: \mathcal{F}^l \times \mathcal{F}^{|S_i|} \rightarrow \mathcal{F}^t$
	\end{enumerate}
	به گونه‌ای که
	$$D_i(E(X), S_i) = (X_{\sigma_{i,1}}, \ldots, X_{\sigma_{i,t}}): \sigma_i \subseteq R_i$$
\end{definition}

در ادبیات پژوهشی به حالت
$t = 1$
،
\picod
 و به حالت
$t > 1$
حالت
\transf{
کدگذاری اندیس منعطف چند درخواسته
}{Pliable Index Coding with Multiple Requests}
می‌گویند.

\begin{remark}[
تفاوت 
\picod 
 و 
\icod
]
مسئله‌ی 
\autoref{fig:pliablefragouli1}
را در نظر بگیرید. در حالت 
\icod
 حداقل دو ارسال توسط فرستنده مورد نیاز است. ولی در
 \picod
 تنها با یک ارسال هر سه گیرنده ارضا می‌شوند.
 \begin{figure}[H]
 	\centering
 	\includegraphics[width=0.6\linewidth]{figs/ch3/pliable_fragouli1}
 	\caption[
 	تفاوت
 	\icod
 	و
 	\picod
 	]{\cite{pliablefirstpaper}}
 	\label{fig:pliablefragouli1}
 \end{figure}
\end{remark}

\begin{remark}
	نکته بسیار مهمی که در ادبیات پژوهشی 
	\picod
	مخفی است و ممکن است خواننده را دچار اشتباه بسیار بزرگی کند نحوه‌ی تعریف توابع کدگشایی است. برای مثال توابع کدگشایی در
	\cite{song2017polynomialtime}
	به شکل
	$D_i: \mathcal{F}^l \times \mathcal{F}^{|S_i|} \rightarrow \mathcal{F}^t \times [m]^t$
	تعریف می‌شود، یعنی خروجی تابع
	$((X_{\sigma_1}, \sigma_1), \ldots, (X_{\sigma_t}, \sigma_t) )$
	است که به ذخن متباتر میکند که در خروجی تابع کدگشایی گیرنده‌ی 
	$i$
	ام بر اساس ورودی تابع(اطلاعات جانبی گیرنده‌ی 
	$i$
	ام و پیام‌های آن ارسال شده توسط فرستنده) مشخص می‌شود که مقدار
	$\sigma_i$
	ها چقدر است. یعنی گیرنده‌ی
	$i$
	ام بر اساس اطلاعات جانبی و پیام‌های دریافتی خود ممکن است پیام‌های مختلفی را بازیابی کند. این صورت بندی مخصوص مسئله‌ی کدگذاری اندیس بسیار منعطف است که در
	\autoref{def:verypliable}
	معرفی می‌کنیم. در مسئله‌ی
	\picod
	اگر بخواهیم به شکل کاملا دقیق توابع کدگشایی را تعریف کنیم باید 
	$\sigma$
	 را در نام تابع بیان کنیم یعنی تابع کدگشایی گیرنده‌ی
	 $i$
	 ام به شکل:
	 	$$D_{i, \sigma_i}(E(X), S_i) = (X_{\sigma_{i,1}}, \ldots, X_{\sigma_{i,t}}): \sigma_i \subseteq R_i $$
	 	در واقع زمانی که می‌خواهیم توابع کدگشایی را تعریف کنیم باید از پیش بگوییم که دقیقا کدام پیام‌ها کدگشایی خواهد شد.
\end{remark}

\begin{note}
	برای حالتی که
	$t > 1$
	است اگر
	$i$
	ای وجود داشته باشد که
	$|R_i| < t$
	در این صورت کافی است گیرنده‌ی 
	$i$
	ام
	$|R_i|$
	پیام را بازیابی کنید.
	\cite{pliablefirstpaper}
\end{note}

\begin{note}
	در 
	\cite{pliablefirstpaper}
	که اولین مقاله این موضوع است به جای استفاده از
	\picodt
	برای حالت چند درخواسته از
	\lr{$k$-\picod}
	استفاده می‌شود.
\end{note}
\begin{notation}[
	گراف اطلاعات جانبی
	]
	همان طور که در
	\autoref{notation:graph1}
	دیدم نمایش‌های گوناگوبی برای مسئله وجود دارد. در این جا ما از گراف اطلاعات جانبی استفاده می‌کنیم. همچنین به مسئله‌ی پیدا کردن کوتاه‌ترین کد اندیس منعطف برای گراف داده شده‌ی 
	$G$
	،
	\picodg
	می‌گوییم.
\end{notation}


در ادامه روی کدهای خطی و همچنین برای راحتی با میدان
$\mathcal{F}_2$
تمرکز می‌کنیم.

\subsection{
\lpicod
،
\nphard
 است
}

\begin{lemma}
برای یک گراف
$G$
داده شده برای هر
$t$
ای همیشه
 $\text{\icodg} > \text{\picodg}$
 است.
 \cite{pliablefirstpaper}
\end{lemma}
\begin{proof}
	یک 
	\icod
	در نظر بگیرید. این کد یک
	\picod
	نیز هست. زیرا فرستنده با استفاده از همان تابع کدگذاری پیام‌ها را ارسال میکند و گیرنده‌ی
	$i$
	با استفاده از تابغ کدگشایی خود می‌تواند دقیقا	 پیام
	$i$
	ام را بازیابی کند. در واقع در
	\picod
	باید تمام حالت‌های ممکن برای بازیابی هر پیامی برای هر گیرنده را در نظر بگیریم و بین همه‌ی آن‌ها کد با کمترین طول را انتخاب کنیم که 
	\icod
	فقط یکی از آن حالت‌ها است.
\end{proof}

در ادامه نشان می‌دهیم که
\transf{
 کدگذاری اندیس منعطف خطی
}{
Linear Pliable index coding, \lpicod}

\nphard
است. برای این کار، 
\lpicod
 را به
\transf{
سه-صدق پذیری همگن یک درست
}{
MONOTONE-1in3-SAT
}
کاهش می‌دهیم.

\begin{definition}
	برای یک مصداق از مسئله‌ی 
	\transf{
	صدق پذیری
}{3SAT}
که تمام اتم‌ها بدون نقیض ضاهر شده اند، مسئله‌ی سه-صدق پذیری همگن یک درست این است که آیا یک مقدار دهی به اتم‌ها وجود دارد به صورتی که اولا عبارت را ارضا کند و دوما در هر جمله(که تمام جملات حاوی سه اتم اند) دقیقا یک اتم صادق باشد
\end{definition}

\namef{
شیفر در
}{Schaefer}
\cite{10.1145/800133.804350}
نشان می‌دهد که مسئله‌ی بالا، 
\nphard
است.

فرض کنید
$\phi$
عبارتی با اتم‌های
$\alpha_1, \ldots, \alpha_M$
و
$N_0$
جمله به شکل روبرو باشد:
$$\phi(\alpha_1, \ldots, \alpha_M) = \bigwedge\limits_{i = 1}^{N_0} CL_i, CL_i = (\alpha_{i, 1} \vee \alpha_{i, 2} \vee \alpha_{i, 3})$$

لم زیر کاهش به مسئله‌ی
\picod
را نشان می‌دهد:
\begin{lemma}
	برای یک نمونه از مسئله‌ی سه-صدق پذیری همگن یک درست مانند
	$\phi$
	یک نمونه از مسئله‌ی
	\picod
	مانند
	$I_{\phi, M, N_0}$
	وجود دارد به گونه‌ای که 
	$\phi$
	صدق پذیر است اگر و تنها اگر 
	$I_{\phi, M, N_0}$
	کدی خطی با طول یک داشته باشد.
\end{lemma}
\begin{proof}
	برای
	$\phi$
	داده شده
		$I_{\phi, M, N_0}$
		را به این شکل می‌سازیم:
		\begin{enumerate}
			\item
			$N_0$
			گیرنده
			$c_i, i \in [N_0]$
			که متناظر جمله‌ی
			$i$
			هستند.
			\item
			$M$
			متغییر تصادفی
			$b_i, i \in [M]$
			 که متناظر اتم‌ها اند.
			 \item 
			 اطلاعات جانبی گیرنده‌ی
			 $i$
			 به شکل زیر است:
			 $$S_i = {j: \alpha_j \notin CL_i}$$
			 یعنی هر گیرنده
			 $M - 3$
			 پیام را به صورت اطلاعات جانبی می‌داند.
		\end{enumerate}
		($\Rightarrow$):
		فرض کنید کدی خطی با طول یک برای
			$I_{\phi, M, N_0}$
			وجود داشته باشد. پس پیامی که فرستنده ارسال می‌کند به شکل
			$Y = (Y_1) = (b_{\sigma_1} \oplus \ldots \oplus b_{\sigma_s}), \sigma_i \in [M]$
			قرار دهید: 
			$\sigma = \{ \sigma_1, \ldots, \sigma_s \}$
			چون تمام گیرنده‌ها پیام جدیدی را با دریافت تنها یک پیام بازیابی کرده اند پس
			$\forall i \in [N_0]: \exists \sigma_t \in \sigma: \sigma_t \notin S_i$
			و چون باید بتوان با 
			$Y_1$
			دریافت شده پیامی را بازیابی کرد باید دقیقا یک
			$\sigma_t$
			به ازای هر
			$i$
			وجود داشته باشد. حال اگر در
			$\phi$
			در جمله
			$i$
			ام اتم
			$\alpha_{\sigma_t}$
			را صادق قرار دهیم تمام جملات که متناظر گیرنده‌ها هستند صادق می‌شوند و در هر جمله دقیقا یک اتم صادق است. در نتیجه یک کد خطی با طول یک را میتوان برای تولید یک مقدار دهی صادقی برای
			$\phi$
			استفاده کرد.
			
			($\Leftarrow$):
			دقیقا مشابه برهان قسمت قبلی می‌توان یک کد خطی با طول یک بر اساس یک مقدار دهی صادق است.
\end{proof}
چون سه-صدق پذیری همگن یک درست یک مسئله‌ی
\nphard
است، طبق لم قبلی پیدا کردن کد با کمترین طول برای
\lpicod
نیز
\nphard
است.
\subsection{
الگوریتم‌های تصادفی
}
\subsubsection{
	\lr{GRCOV1} 
}
\picod
ذات بسیار متفاوتی نسبت به
\icod
دارد. مثلا زمانی که یکی از متغیرها در مجموعه‌ی اطلاعات جانبی هیچ کدام از گیرنده‌ها نباشد تنها با یک ارسال می‌توان تمام گیرنده‌ها را ارضا کرد در حالی که در
\icod
این کد با طول یک، کد درستی نیست. در واقع بر خلاف
\icod
که هر تک گیرنده ممکن است به خاطر پیامی که می‌خواهد بازیابی کند طول کد را افزایش دهد در
\picod
گیرنده‌ها برهمکنش بیشتری روی طول کد دارند. به عنوان مثالی دیگر زمانی که اطلاعات جانبی تعدادی از گیرنده‌ها شامل تمام متغیرها به جز یکی است فرشتنده کافی است مجموع تمام متغیرها را ارسال کند. 

ایده‌ی اصلی الگوریتم‌های
\lr{GRCOV1 \& GRCOV2}
 پیدا کردن مجموعه‌هایی از گیرنده‌ها است که ویژگی‌های بالا را داشته باشند.
 
 \begin{remark}
 	در مقاله
 	\cite{pliablefirstpaper}
 	نمادگذاری گراف اطلاعات جانبی بر خلاف نمادگذاری مرسوم این حوزه و این پایان نامه است. در واقع در مقالات بعدی این نمادگذاری تغییر کرد. در این مقاله اگر
 	$j \notin S_i$
 	آنگاه بین
 	$b_j$
 	و
 	$c_i$
 	یالی قرار دارد که برعکس نمادگذاری مرسوم است.
 \end{remark}
 
 فرض کنید برای دو متغیر
 $b_{\sigma_1}$
 و
 $\b_{\sigma_2}$
 \footnote{
 همان طور که در
 \autoref{remark:xbdiff}
 گفته شد 
 $b_{\sigma_1}$
 واقعا متغیر تصادفی نیست بلکه راس متناظر متغیر
 $X_{\sigma_1}$
 است.
 }
 همسایه‌های این دو راس
 $N[b_{\sigma_1}] \cup N[b_{\sigma_2}]$
 را بر اساس تعداد همسابه‌های خود در بخش
 $B$
 گراف به دو دسته تقسیم می‌کنیم. دسته اول راس‌های
 $c_i$
 ای که به همه به جز یکی وصل اند. برای مجموعه‌ی
 $B_1 \subseteq B$
 مجموعه‌ی
 $W_i(B_1) \subseteq C$
 تمام همسایه‌های
 $B_1$
 اند( که چون گراف دو بخشی است حتما در$C$ می‌افتند) که به 
 $|B_1| - i$
 راس از راس‌های
 $B_1$
  وصل اند. حال اگر
  $Y_1 = \bigoplus\limits_{b \in B_1} b$
  را فرستنده ارسال کند تمام اعضای
  $W_1(B_1)$
  میتوانند با اطلاعات جانبی خود پیام جدیدی را با کم کردن اطلاعات جانبی خود از
  $Y_1$
  بازیابی کنند.
  
 با توجه به بحث قبلی در الگوریتم
\GRCOVone
به دنبال مجموعه‌ای از رئوس گراف مانند
$B_1$
هستیم که 
$|W_1(B_1)|$
بیشینه باشد. به جای اینکه در هر گام مجموعه‌ای با اندازه‌ی ماکسیمم پیدا کنیم به صورت حریضانه مجموعه‌ای با اندازه‌ی ماکسیمال پیدا می‌کنیم.

به مجموعه‌ی 
$B_1 = \{b_{v_1}, \ldots, b_{v_t}\}$
می‌گوییم ماکسیمال اگر برای هر راس
$b_{v_{t  +1}} \notin B_1$
داشته باشیم:
\begin{equation}
	\tag{شرط ماکسیمال بودن}
	|W_1(B_1 \cup \{b_{v_{t + 1}} \})| < |W_1(B_1)|
\end{equation}

برای پیدا کردن یک مجموعه‌ی ماکسیمال با یک مجموعه‌ی تهی شروع می‌کنیم و در هر گام یک راس جدید را به صورت حریصانه به طوری که 
$|W_1(B_1)|$
را بیشینه کند انتخاب میکنیم. این کار را تا زمانی که نتوانیم ادامه بدهیم انجام می‌دهیم.

زمانی که مجموعه‌ی ماکسیمال را پیدا کردیم مطابق آنچه قبل تر گفته شد پیام
 $Y_1 = \bigoplus\limits_{b \in B_1} b$
 را به کد خود اضافه می‌کنیم(فرستنده این پیام را هم ارسال می‌کند). سپس تمام راس‌های
 $B_1$
 و
 $w_1(B_1)$
 از گراف حذف می‌شوند و دوباره همین کار را تکرار می‌کنیم. در نهایت پس از 
 $s$
 گام کد نهایی برابر
 $Y = (Y_1, \ldots, Y_s), Y_i =  \bigoplus\limits_{b \in B_i} b$
 خواهد بود. این الگوریتم
 \GRCOVone\footnote{\lr{greedy cover}}
 	نام دارد. زمان اجرای این الگوریتم برابر
 	$O(mn^2)$
 	است.
 	\begin{algorithm}[H]
 		\caption{GrCov1($G,m,n,t$) \cite{pliable2015paper}}
 		\label{algorithm:grcov1}
 		\begin{algorithmic}[1]
 			\State G is a \picodt with n clients and m messages
 			\State $\mathcal{C} = \{\}$
 			\State $\forall i \in [n]: CNT[i] = t$ 
 			\While{$\exists i$ s.t. $CNT[i] \neq 0$}
		 			\State $B \leftarrow \varnothing$.
		 			\While{$B$ is not a maximal set}
				 			\State Find message vertex $b_v \notin B$ such that $\left|W_1\left(B \cup\left\{b_v\right\}\right)\right|$ is maximized.
				 			\State $B \leftarrow B \cup\left\{b_v\right\}$
		 			\EndWhile
		 			\State $\mathcal{C} \leftarrow \mathcal{C} \cup\left\{\sum_{u=1}^{|B|} b_{v_u}, b_{v_u} \in B\right\}$
		 			\For{ $c_i \in W_1(B)$ }
		 			\State If $c_i$ is connected to $b_j \in B$, then delete the correspond-ing edge in $G$.
		 			\State $CNT[i] \leftarrow CNT[i]-1$.
		 			\EndFor
 			\EndWhile
 			\State Output: $\mathcal{C}$
 		\end{algorithmic}
 	\end{algorithm}
 \subsubsection{
 	\lr{GRCOV2} 
 }
 این الگوریتم تغییر کوچکی نسبت به الگوریتم اصلی دارد. در الگوریتم اصلی وقتی یک مجموعه‌ی ماکسیمال را پیدا می‌کردیم هم
 $B_1$
 و هم متغیرهای مربوطه یعنی
 $w_1(B_1)$
 را حذف می‌کردیم. ولی ممکن است که این متغیرها در پیدا کردن مجموعه‌های ماکسیمال بهتری به ما کمک کنند. در این الگوریتم تنها
 $B_1$
 را حذف می‌کنیم.
 
 زمان اجرای این الگوریتم نیز
 $O(mn^2)$
 است.
 	\begin{algorithm}[H]
 	\caption{GrCov2($G,m,n,t$) \cite{pliablefirstpaper}}
 	\label{algorithm:grcov2}
 	\begin{algorithmic}[1]
 		\State G is a \picodt with n clients and m messages
 		\State  $U C \leftarrow\left\{c_1, \cdots, c_n\right\}$,
 		\State  num\_bits $\leftarrow 0, \mathcal{C}=\{\}$.
 		\While{$U C \neq \varnothing$}
		 		\State $B \leftarrow \varnothing$.
		 		\While{$B$ is not a maximal set}
				 		\State  Find bit vertex $b_v$ such that $\left|W\left(B \cup\left\{b_v\right\}\right)\right|$ is maximized.
				 		\State  $B \leftarrow B \cup\left\{b_v\right\}$.
		 		\EndWhile
		 		\State  $UC \leftarrow UC \backslash w_1(B)$.
		 		\State Remove $w_1(B)$ and all edges connected to it from $G$.
		 		\State  $\mathcal{C} \leftarrow \mathcal{C} \cup\left\{\bigoplus_{t=1}^{|B|} b_{v_t}, b_{v_t} \in B\right\}$
		 		\State  num\_bits $\stackrel{t=1}{\leftarrow}$ num\_bits +1 .
 		\EndWhile
 		\State  Output $\mathcal{C}$,num\_bits.
 	\end{algorithmic}
 \end{algorithm} 
 \subsection{\lr{SETCOV}}
 فرگولی و براهما برای آزمایش کارایی الگوریتم‌های بالا، الگوریتم
 \lr{SETCOV}
 را معرفی می‌کند. این الگوریتم بر مبنای کاهش مسئله به
 \icod
 طراحی شده است. در 
 \picod
 هر گیرنده 
 $i$
 با بازیابی هر پیام از
 $R_i$
 ارضا می‌شود. برای تبدیل یک نمونه از
 \picod
 به
 \icod
 به ازای هر گیرنده
 $R_i$
 شبه‌گیرنده 
 $c_{i, 1}, \ldots, c_{i, {R_i}}$
 قرار می‌دهیم که هر کدام به دنبال یکی پیام‌های
 $R_i$
 است و اطلاعات جانبی مشابهی مانند
 $c_i$
 دارد. در نتیجه 
 \icod
 نهایی 
 $\sum\limits_{i = 1}^{n} |R_i|$
 گیرنده خواهد داشت. این نمونه توسط هر کدام از الگوریتم‌های
 \icod
 قابل حل است. فرگولی از بین الگوریتم‌های ارائه شده در
 \cite{25}
 ساده‌ترین آن‌ها که بر مبنای
 \transf{
 	پوشش خوشه‌ای حریصانه
 	}{
 greedy clique cover
 }
 است را استفاده می‌کند.
 
 پس از حل مسئله‌ی
 \icod
 ایجاد شده فرض کنید ی
 $Y$
 کد خروجی الگوریتم پوشش خوشه‌ای حریصانه باشد. در این الگوریتم هر گیرنده تنها با استفاده از یک از پیام‌ها، پیام مورد نظر خود را بازیابی می‌کند. در واقع هر پیام توانایی بازیابی تنها با استفاده از همان پیام را به تعداد از گیرنده‌ها می‌دهد. این مسئله باعث می‌شود به طورر طبیعی یک رابطه‌ی "پوشش" دادن بین اعضای کد و گیرنده‌ها(در اینجا شبه‌گیرنده‌ها) ایجاد شود به این صورت که هر عضو کد تعداد از گیرنده‌ها را پوشش می‌دهد. چون هدف اصلی ما گیرنده‌های
 \picod
 هستند و نه شبه‌گیرنده‌هایی که ساخته ایم این برای هر عضو کد پوشش را به صورت گیرنده‌هایی که ارضا می‌کند تعریف می‌کنیم. یعنی اگر
 $Y_t$
 شبه‌گیرنده‌های
 $c_{\sigma_1, g_1}, \ldots, c_{\sigma_s, g_s}$
 را ارضا می‌کند تعریف می‌کنیم:
 $$C(Y_t) =\{ c_{\sigma_1}, c_{\sigma_1}\}$$
 طبیعی است که هر گیرنده ممکن است در چندین مجموعه‌ی مختلف باشد. برای پیدا کردن یک جواب برای
 \picod
 تنها کافی است تعداد از 
 $C(Y_i)$
 ها را پیدا کنیم که کل گیرنده‌ها را پوشش دهند و در بین جواب‌ها دنبال جواب با کمترین تعداد مجموعه‌ ایم. این یک نمونه از مسئله‌ی معروف
 \transf{
 پوشش مجموعه‌ای
 }{SET-COVER}
 است که عناصر، گیرنده‌ها هستند و مجموعه‌ها پوشش‌های
 $C(Y_i)$.
 
 فرگولی در پیاده سازی آزمایشگاهی از روش استاندارد حریصانه تقریبی برای این مسئله استفاده می‌کند که زمان اجرای نهایی آن
 $O(m^2 n^6)$
 است.
\subsubsection{k-GRCOV2}
برای حالت
\picodt
یک تعمیم طبیعی از الگوریتم قبلی وجود دارد. به جای اینکه در هر گاه راس‌هایی که یک پیام جدید را بازیابی می‌کنند حذف کنیم، هر راس 
$c_i$
یک شمارنده برابر
$\min(k, |R_i|)$
دارد و در هر گام که یک راس یک پیام جدید را بازیابی می‌کند این شمارنده را یک واحد کم می‌کنیم و راس‌های
$w_1(B_j)$
را به راس
$c_i$
به عنوان اطلاعات جانبی وصل می‌کنیم. هر راسی که شمارنده‌اش به صفر برسد را هم حذف می‌کنیم. زمان اجرای این الگوریتم
$O(kmn^2)$
است.

\subsubsection{
نتایج عددی
}
فرگولی و براهما  الگوریتم‌های گفته شده را به صورت عملی روی نمونه‌های تصادفی از مسئله تست می‌کنند. در این آزمایش‌ها 
$n = m = 500$
و هر یال در گراف اطلاعات جانبی را هم با احتمال
$p$
قرار می‌دهند. یعنی هر گیرنده با احتمال
$p$
ان پیام‌ را از قبل می‌داند. همچنین برای مقایسه بهتر، در نمونه‌های تولیدی هر گیرنده می‌خواهد یک پیام متفاوت را در مسئله‌ی
\icod
بازیابی کند.
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{figs/ch3/pliable2012}
	\caption{\cite{pliablefirstpaper}}
	\label{fig:pliable2012}
\end{figure}

در 
\autoref{fig:pliable2012}
کاراریی الگوریتم‌های مختلف به ازای 
$p$
های مختلف آمده است. برای هر
$p$
بیش از هزار بار آزمایش تکرار شده است. همان طور که در شکل مشخص است تفاوتی چشمگیر بین الگوریتم‌های
\picod
و الگوریتم‌های مختلف
\icod
است(تفاوتی چشمگیری بین الگوریتم‌های مختلف
\icod
که در این مقایسه مهم باشد دیده نمی‌شود). در حالی که الگوریتم‌های گفته شده‌ی
\picod
به طور میانگین کمتر از
$11$
پیام ارسال می‌کنند الگوریتم‌های
\icod
تنها وقتی
$p \geq 0.95$
است این کاراریی را دارد. یعنی زمانی که گراف اطلاعات جانبی بسیار پر است.

در بین الگوریتم‌های بیان شده
\lr{GRCOV2}
بهترین عملکرد را دارد و
\lr{GRCOV1}
با فاصله‌ی کمی از آن عمل می‌کند.

الگوریتم
\lr{SETCOC}
روند جالبی دارد. در محدوده‌ی 
$p \leq 0.5$
بهتر از
\lr{GRCOV2}
عمل می‌کند. بخشی از این عملکرد را می‌توان این گونه توجیه کرد که برای مقادیر کم
$p$
گراف اطلاعات جانبی تنک می‌شود و در نتیجه هر عضو کد(پیام ارسال شده توسط فرستنده)  برای تعداد بیشتری گیرنده حاوی اطلاعات(آنتروپی شرطی(بر پایه اطلاعات جانبی هر گیرنده)) است و در نتیجه وقتی به دنبال پوشش هستیم هر مجموعه گیرنده‌های بیشتری را شامل می‌شود.

همان طور که در
\autoref{fig:pliable2012}
می‌بینید بیشینه‌ی طول کد در الگوریتم
\lr{GRCOV2}
در آزمایش‌های انجام شده برای تمام نمونه‌های
\picod
کمتر از
$\log(500) \approx 6$
است. در شکل
\autoref{fig:pliable20122}
به ازای مقادیر محتلف 
$n$
(که همچنان
$m = n$
) و نمونه‌های تصادفی یونیفرم می‌بینیم که برای
$n \geq 60$
طول کد حداکثر
$\log(n)$
است. فرگولی و براهما در این مقاله حدس می‌زنند که در حالت کلی این حکم برقرار باشد که نشان از یک تفاوت از مرتبه توانی بین طول کد
\picod
و 
\icod
است. این حدس بعدا توسط همین دو نفر در
\cite{pliable2015paper}
اثبات می‌شود که در
\autoref{}
خواهیم دید. 

نتایج اجرای الگوریتم
\lr{k-GRCOV2}
در 
\autoref{fig:pliable20123}
آمده است. همان طور که مشخص است در عمل به طور میانگین با کدهای با طول کمتر از
$17$
می‌توان 
$500$
گیرنده که به دنبال بازیابی 
$5$
هستند را ارضا کرد.
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{figs/ch3/pliable2012_2}
	\caption{\cite{pliablefirstpaper}}
	\label{fig:pliable20122}
\end{figure}
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{figs/ch3/pliable2012_3}
	\caption{\cite{pliablefirstpaper}}
	\label{fig:pliable20123}
\end{figure}

\begin{lemma}
	اگر
	$C' \subseteq C$
	مجموعه‌ای شامل 
	$k$
	تا از گیرنده‌‌ها باشد. بدون از دست رفتن کلیت فرض می‌کنیم
	$C' = \{c_1, \ldots, c_k\}$
	. و تعریف می‌کنیم:
	$d_{max} = max\{R_i: i \in [k] \}$
	و
	$d_{min} = min\{R_i: i \in [k] \}$
	و همچنین فرض می‌کنیم که هر گیرنده حداقل به دنبال یک پیام هست یعنی
	$d_{min} \geq 1$
	. اگر ثابت
	$r \geq 1$
	وجود داشته باشد که
	$d_{max} \leq r d_{min}$
	در این صورت کدی با اندازه‌ی
	$O(\log(k))$
	وجود دارد که تمام اعضای
	$C'$
	را ارضا کند.
\end{lemma}
\begin{proof}
	برای نشان دادن حکم از یک اثبات احتمالاتی استفاده می‌کنیم.
	
	فرض کنید که
	$B_0$
	پیام‌هایی باشند که اطلاعات جانبی اعضای
	$C$
	نیستند یعنی:
	$B_0 = \cup_{i = 1}^{k} R_i$
	. اگر
	$d_{max} = 1$
	باشد در این صورت ارسال
	$Y_1 = \sum_{b \in B_0} b$
	تمام گیرنده‌های
	$C'$
	را ارضا می‌کند که کدی با طول یک به ما می‌دهد. اگر
	$d_max \geq 2$
	بود زیرمجموعه‌ی
	$B_1 \subseteq B_0$
	را به این صورت می‌سازیم.	به صورت تصادفی هر عضو
	$B_0$
	را با احتمال
	$1 - p$
	در 
	$B_1 $
	قرار می‌دهیم.
	
	احتمال اینکه گیرنده‌ی
	$c_i$
	به دقیقا یکی از اعضای 
	$B_1$
	وصل نباشد برابر است با
	$$P_i = R(c_i) p (1 - p)^{R(c_i) - 1}$$
	توجه کنید که راس
	$c_i$
	به تمام راس‌های
	$B_0 \setminus R_i$
	و همچنین
	$R_i \subseteq B_0$
	وصل است پس تنها باید یک راس از
	$R_i$
	انتخاب شود. در نتیجه امید ریاضی اندازه‌ی
	$w_1(B_1)$
	برابر است با
	\begin{equation}
		E[|W_1(B_1)] \geq k \dfrac{d_{min}}{max}(1 - \dfrac{1}{d_{max}})^{d_{max} - 1} \geq k \dfrac{d_{min}}{max} \geq \dfrac{k}{\gamma r}
	\end{equation}
	که منظور از
	$\gamma$
	\transf{ثابت اویلر}{Euler's constant, Euler–Mascheroni constant}
	است. چون حتما یک مجموعه‌ی
	$B_1$
	وجود دارد که از 
	$|W_1(B_1)|$
	از امیدریاضی بیشتر مساوی باشد. در نتیجه ارسال جمع اعضای
	$B_1$
	کسر ثابتی
	$k$
	گیرنده‌ را ارضا می‌کند و حداکثر
	$k(1 - \dfrac{1}{\gamma r})$
	گیرنده می‌ماند. حال اگر گیرنده‌های ارضا شده را از گراف حذف کنیم چون راس‌های متناظر با پیام‌ها تغییری نمی‌کنند پس همچنان در گراف باقی مانده
	$d_{max} \leq r d_{min}$
	. در نتیجه عمل بالا را باز هم می‌توانیم تکرار کنیم تا زمانی که تعداد ثابتی از گیرنده‌ها باقی بماند(مثلا یک گیرنده). برای هر گیرنده‌ی باقی مانده یکی از پیام‌هایی که ندارد را ارسال می‌کنیم. چون در هر گام تعداد گیرنده‌های باقی مانده با یک ضریب کم می‌شود حداکثر در
	$O(\log(k))$
	مرحله این تکرار تمام می‌شود و کدی با طول
	$O(\log(k))$
	به دست می‌آید.
\end{proof}

\begin{remark}
	در عمل اگر 
	$\exists s: \forall i \in [n]: S_i = s$
	در این صورت کدی با طول
	$O(\log(n))$
	وجود دارد که تمام گیرنده‌ها را ارضا می‌کند. این یک بهبود توانی نسبت به
	$\Omega(n)$
	در
	\icod
	در بدترین حالت برای
	$m = n$
	است.
\end{remark}
برای زمانی که اطلاعات جانبی گیرنده‌ها دلخواه است، گیرنده‌ها را به بخش‌های مناسب تقسیم می‌کنیم و سپس از لم قبلی استفاده می‌کنیم.
\begin{theorem}
	برای هر نمونه از
	\picodtt{1}
	با
	$n$
	گیرنده و 
	$m$
	پیام کدی با طول
	$$O(\min\{m, n, \log(m(1 + \log^{+}(\dfrac{n}{\log(m)}) ))\})$$
	وجود دارد.
\end{theorem}
\begin{proof}
	درجه راس‌های 
	$c_i$
	گراف می‌تواند
	$0$
	تا
	$m - 1$
	باشد. این رئوس را به دسته‌ها
	$S_{i}' = \{c_l: m - 2^{i - 1} \geq d(c_l) > m -  2^i\}$
	تقسیم می‌کنیم. برای راحتی مجموعه‌های تهی را دور میریزیم و بقیه را
	$S_1, \ldots, S_g$
	نام‌گذاری می‌کنیم. به وضوج شرط لم قبلی در تمام این مجموعه‌ها با
	$r = 2$
	برقرار است و همچنین
	$g \leq 1 + \lceil\log_2(m)\rceil$
	. حال این مجموعه‌ها را به دو گروه تقسیم می‌کنیم.
	$G_1 = \{ S_i: |S_i| < 3\}$
	و 
	$G_2 = \{ S_i: |S_i| \geq 3\}$
	چون
	$\sum_{i \in G_2} |S_i| \leq \sum_{i \in [g]} |S_i| = n$
	پس
	$|G_2| \leq \dfrac{n}{3}$
	. از طرفی
	$|G_1| + |G_2| = g$
	. حال طبق لم قبلی به ازای یک ثابت
	$K_1$
	به
	$K_1(1 + \log(|S_i|))$
	پیام برای ارضا اعضای
	$S_i$
	نیاز داریم که یک اضافی در جمله داخل پرانتز برای حالتی است که
	$|S_i| = 1$
	. در نتیجه در نهایت طول کد برابر می‌شود با:
	\begin{align}
		K_1 \sum\limits_{i = 1}^{g} (1 + \log(|S_i|)) 
		 & \leq K_1 \sum_{i \in G_1}	(1 + \log(|S_i|)) + K_1 \sum_{i \in G_2} (1 + \log(|S_i|)) \\
		& \leq K_1 |G_1| (1 + \log(2)) + K_1 |G_2| + K_1 |G_2| \log(\dfrac{\sum_{i\in G_2} |S_i}{|G_2}) \\
		& \leq q K_1 |G_1| (1 + \log(2)) + K_1 |G_2| + K_1 |G_2| \log(\dfrac{n}{|G_2|}) \\
		& = O(\log m (1 + \log^{+}(\dfrac{n}{\log m})))
	\end{align}
	نامساوی دوم از 
	\hyperref[Jensen]{
	نامساوی ینسن
	}
	روی تابع لگاریتم و این نکته که
	$\forall i in G1: |S_i| < 3$
	نتیجه می‌شود. آخرین خط این گونه توجیه می‌شود که تابع
	$x \log(n/x)$
	روی
	$x \leq n/e$
	ناکاهشی است در نتیجه اگر
	$1 + \lceil \log m \rceil \leq n/3$
	باشد چون
	$|G_2| \leq g$
	داریم
	$$|G_2| \log (\dfrac{n}{|G_2|}) = O(\log m \log (\dfrac{n}{\log m}))$$
	و اگر برعکس این حالت برقرار باشد یعنی
	$1 + \lceil \log m \rceil \ge n/3$
	چون 
	$|G_2| \leq n/3$
	داریم:
	$$|G_2| \log(\dfrac{n}{|G_2}) = \dfrac{n}{3} \log(3) = O(\log m)$$
\end{proof}

همچنین اگر تمام پیام‌ها را ارسال کنیم(
$m$
ارسال) تمام گیرنده‌ها می‌توانند پیام جدیدی را بازیابی کنند. همچنین اگر به ازای هر گیرنده یک پیام خاص را ارسال کنیم باز هم این برقرار است. در نتیجه با کمینه گرفتن از این سه نتیجه، قضیه اثبات می‌شود.
\subsubsection{\lr{RANDCOV}}
\subsection{الگوریتم قطعی}
\subsubsection{\lr{BinGreedy}}











