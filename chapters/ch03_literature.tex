% !TeX spellcheck = fa-IR
\chapter{کارهای پیشین}
\label{chapter:literature}
در این فصل پژوهش‌های انجام شده بر روی مسئله‌ی کدگذاری اندیس منعطف را بررسی می‌کنیم.

همان‌طور که در فصل اول گفته شد، \picod توسط براهما و فرگولی در سال 2015 در
\cite{pliablefirstpaper}
به عنوان توسعه‌ای از \icod معرفی شد. در این مسئله هر گیرنده به جای این‌که به دنبال یک پیام خاص باشد، به دنبال یک پیام جدید است که در مجموعه‌ی اطلاعات جانبی‌‌اش نباشد.

این مسئله کاربردهای مختلفی در مسائل حوزه‌های گوناگون دارد. برای مثال در محاسبات توزیع‌شده
\cite{datashuf}،
\transf{سیستم‌های پیشنهاد دهنده}{recommendation system}
\cite{8404065}
و
\transf{یادگیری توزیع شده}{distributed learning}
\cite{8682270}. در این فصل به مرور ادبیات پژوهشی
\picod
می‌پردازیم.

پس از تعریف دقیق مسئله، نشان می‌دهیم که
\picod
\nphard
است. پس از آن الگوریتم‌های
\lr{GRCOV}،
\lr{SETCOV}
و
\lr{RANDCOV}
را در حالت‌های تک‌ارسال و چندارسال بررسی می‌کنیم. در نهایت به بررسی الگوریتم
\lr{BinGreedy}
که یک
\transf{الگوریتم تقریبی قطعی}{Deterministic Approximation Algorithm}
است می‌پردازیم.

در این فصل با بررسی مقالات مختلفی مثل
\cite{pliablefirstpaper,6620405, pliable2015paper,  song2017polynomialtime}
(نسخه ژورنالی مقاله دوم
\cite{pliable2016})
از فرگولی و همکاران به تعریف دقیق مسئله می‌پردازیم. فرگولی در این مقالات با تعریف مسئله برای اولین بار، به مرور نتایج مختلفی را اثبات می‌کند.


\section{تعریف دقیق مسئله}
\label{sec:3:3}
سیستمی با یک فرستنده و
$n$
گیرنده در نظر بگیرید. فرستنده، مقدار
$m$
متغیر تصادفی
$X = (X_1, \ldots, X_m): X_i \in \mathbb{F}$
مشاهده شده را در دست دارد. هر گیرنده‌ای مانند
$i$، بخشی از متغیرهای تصادفی را در اختیار دارد که اندیس آن‌ها را با
$S_i \subseteq [m]$
نشان می‌دهیم. هر گیرنده می‌خواهد
$t$
پیام که جزء اطلاعات جانبی‌اش نیست یعنی اندیسشان در
$R_i = [m] \setminus S_i$
باشد را بازیابی کند.

فرستنده می‌تواند تعدادی پیام مانند
$Y = (Y_1, \ldots, Y_l): Y_i \in \fq$
را برای تمام گیرنده‌ها ارسال کند تا گیرنده‌ها بتوانند با پیام‌های ارسالی و اطلاعات جانبی خود پیام‌های مورد نظر خود را بازیابی کنند. در این مسئله گیرنده‌ها به دنبال پیام‌هایی هستند که که در مجموعه‌ی اطلاعات جانبی‌شان نباشند. در
\picod
قبل از ارسال پیام‌ها توسط فرستنده، بر اساس مجموعه‌های اطلاعات جانبی گیرنده‌ها، پیام‌هایی که توسط هر گیرنده بازیابی خواهند شد مشخص می‌شود. برخلاف
\icod
که هر گیرنده پیامی مشخص را باید بازیابی کند، در این‌جا مشخص کردن پیام‌هایی که بازیابی می‌شوند نیز بخشی از مسئله است.

مشابه
\autoref{def:icod}
به تعریف رسمی
\picodt
میپردازیم:
\begin{definition}[\picodt]
    \label{def:picodt}
    برای
    $n$
    و
    $m$
    و
    $S_i$های داده‌شده‌ی یک
    \picodt
    عبارت است از:
    \begin{enumerate}
        \item
        تابع کدگذاری
        $E: \ff^n \rightarrow \ff^l$
        که به
        $l$
        طول کد می‌گوییم.
        \item متغیرهای هدف به ازای هر گیرنده
        $\sigma_i \subseteq R_i,  \card{\sigma_i} = t$
        \item
        توابع کدگشایی
        $D_i: \ff^l \times \ff^{\card{S_i}} \rightarrow \ff^t$
    \end{enumerate}
    به گونه‌ای که
    $$D_i(E(X), S_i) = (X_{\sigma_{i,1}}, \ldots, X_{\sigma_{i,t}}): \sigma_i \subseteq R_i$$
\end{definition}

در ادبیات پژوهشی به حالت
$t > 1$
\transf{کدگذاری اندیس منعطف چند درخواسته}{Pliable Index Coding with Multiple Requests}
می‌گویند.

مسئله
\picodt
پیدا کردن الگوریتمی است که با گرفتن
$S_i$
ها،
$\sigma$
و
$Y$
را پیدا کند به طوری که هر گیرنده بتواند
$t$
پیام جدید را بازیابی کند و
$l = \card{Y}$
کمینه شود.

\begin{remark}[تفاوت
\picod
و
\icods]
    مسئله‌ی
    \autoref{fig:pliablefragouli1}
    را در نظر بگیرید.
     \begin{figure}
    	\begin{subfigure}[b]{0.45\textwidth}
    		\centering
    		\begin{tikzpicture}[->, >=stealth, auto, semithick]
    			% Set the positions of the nodes
    			\node[circle, draw=blue, fill=blue!20, inner sep=0pt] (C1) at (2,2) {$C_1$};
    			\node[circle, draw=blue, fill=blue!20, inner sep=0pt] (C2) at (3.5,2) {
    				$C_2$
    				
    			};
    			\node[circle, draw=blue, fill=blue!20, inner sep=0pt] (C3) at (5,2) {
    				$C_3$
    			};
    			
    			\node[circle, draw=green, fill=green!20, inner sep=0pt] (B1) at (2,0) {$B_1$};
    			\node[circle, draw=green, fill=green!20, inner sep=0pt] (B2) at (3.5,0) {$B_2$};
    			\node[circle, draw=green, fill=green!20, inner sep=0pt] (B3) at (5,0) {$B_3$};
    			% Position the parts
    			%			\begin{scope}[on background layer]
    				%				\alt<1->{\node[fit=(C1) (C2) (C3), draw=blue, fill=blue!10, rounded corners] {};}{}
    				%				\alt<1->{\node[fit=(B1) (B2) (B3), draw=green, fill=green!10, rounded corners] {};}{}
    				%			\end{scope}
    		\end{tikzpicture}
    		\caption{\picod}
    	\end{subfigure}
    	\begin{subfigure}[b]{0.45\textwidth}
    		\centering
    		\begin{tikzpicture}[->, >=stealth, auto, semithick]
    			% Set the positions of the nodes
    			
    			\node[circle, draw=blue, fill=blue!20, inner sep=0pt] (C1) at (2,2) {
    				$C_1$
    			};
    			\node[circle, draw=blue, fill=blue!20, inner sep=0pt] (C2) at (3.5,2) {
    				$C_2$
    				
    			};
    			\node[circle, draw=blue, fill=blue!20, inner sep=0pt] (C3) at (5,2){$C_3$                    };
    			
    			
    			\node[circle, draw=green, fill=green!20, inner sep=0pt] (B1) at (2,0) {$B_1$};
    			\node[circle, draw=green, fill=green!20, inner sep=0pt] (B2) at (3.5,0) {$B_2$};
    			\node[circle, draw=green, fill=green!20, inner sep=0pt] (B3) at (5,0) {$B_3$};
    			% Draw the edges
    			\draw[->, color=Red]  (C1) -- (B1);
    			\draw[->, color=Red]  (C2) -- (B2);
    			\draw[->, color=Red]  (C3) -- (B3);
    		\end{tikzpicture}
    		\caption{\icod}
    	\end{subfigure}
    	\caption[تفاوت
    	\icod
    	و
    	\picod
    	]{
    		تفاوت
    		\icod
    		و
    		\picod
    	}
    	\label{fig:pliablefragouli1}
    \end{figure}
    برای
    \icod
    پیام مورد نظر هر گیرنده را با یال قرمز نشان داده شده است. همچنین مجموعه‌ی اطلاعات جانبی گیرنده‌ها در هر دو حالت برای تمام گیرنده‌ها تهی است. در حالت
    \icod
    حداقل سه ارسال توسط فرستنده مورد نیاز است. ولی در
    \picod
    تنها با یک ارسال هر سه گیرنده ارضا می‌شوند.
\end{remark}
\begin{remark}
    \label{remark:pliablevsvery}
    نکته بسیار مهمی که در ادبیات پژوهشی
    \picod
    مخفی است و ممکن است خواننده را دچار اشتباه بسیار بزرگی کند، نحوه‌ی تعریف توابع کدگشایی است. در مقالات این حوزه
    $\sigma$
    را به عنوان بخشی از تعریف اندیس‌کد نمی‌آورند. برای مثال توابع کدگشایی در
    \cite{song2017polynomialtime}
    به شکل
    $D_i: \ff^l \times \ff^{\card{S_i}} \rightarrow \ff^t \times [m]^t$
    تعریف می‌شود، یعنی خروجی تابع
    $\big((X_{\sigma_1}, \sigma_1), \ldots, (X_{\sigma_t}, \sigma_t) \big)$
    است که به ذهن متبادر می‌کند که در خروجی تابع کدگشایی گیرنده‌ی
    $i$-ام، بر اساس ورودی تابع (اطلاعات جانبی گیرنده‌ی
    $i$-ام و پیام‌های آن ارسال شده توسط فرستنده) مشخص می‌شود که مقدار
    $\sigma_i$ها چقدر است. یعنی گیرنده‌ی
    $i$-ام براساس اطلاعات جانبی و پیام‌های دریافتی خود ممکن است پیام‌های مختلفی را بازیابی کند. این صورت‌بندی مخصوص مسئله‌ی دیگری به نام کدگذاری اندیس بسیار منعطف است که در
    \autoref{sec:verypliable}
    معرفی می‌کنیم. در مسئله‌ی
    \picod
    اگر بخواهیم به شکل کاملا دقیق توابع کدگشایی را تعریف کنیم باید
    $\sigma$
    را در نام تابع کدگشایی بیان کنیم، یعنی تابع کدگشایی گیرنده‌ی
    $i$-ام به شکل زیر تعریف می‌شود:
    \begin{align}
        D_{i, \sigma_i}(E(X), S_i) = (X_{\sigma_{i,1}}, \ldots, X_{\sigma_{i,t}}): \sigma_i \subseteq R_i
    \end{align}
    در واقع تعریف توابع کدگشایی نباید به مقدار متغیرهای تصادفی
    $X_i$
    وابسته باشد و زمانی که می‌خواهیم توابع کدگشایی را تعریف کنیم باید از پیش بگوییم که دقیقا کدام پیام کدگشایی خواهد شد.
\end{remark}

\begin{note}
    برای حالتی که
    $t > 1$
    است اگر
    $i$ای وجود داشته باشد که
    $\card{R_i} < t$
    در این صورت کافی است گیرنده‌ی
    $i$ام
    $\card{R_i}$
    پیام را بازیابی کنید \cite{pliablefirstpaper}.
\end{note}

\begin{note}
    در
    \cite{pliablefirstpaper}
    که اولین مقاله این موضوع است به جای استفاده از
    \picodt
    برای حالت چنددرخواسته از
    \lr{$k$-\picod}
    استفاده می‌شود.
\end{note}
\begin{notation}[گراف اطلاعات جانبی]
    همان‌طور که در
    \autoref{notation:graph1}
    دیدیم نمایش‌های گوناگونی برای مسئله وجود دارد. در این‌جا ما از گراف اطلاعات جانبی استفاده می‌کنیم. همچنین به مسئله‌ی پیدا کردن کوتاه ترین اندیس‌کد منعطف برای گراف داده‌شده‌ی
    $G$،
    \picodg
    می‌گوییم.
\end{notation}

قبل از شروع بررسی دقیق \picods، در لم زیر نشان می‌دهیم که جواب \picod را می‌توان به عنوان حالت خاصی از جواب \icod متناظر دید.
\begin{lemma}
    برای یک گراف
    $G$
    داده‌شده برای هر
    $t$ای همیشه
    $\icodg \leq \picodg$
    است \cite{pliablefirstpaper}.
\end{lemma}
\begin{proof}
    یک
    \icod
    در نظر بگیرید. این کد یک
    \picod
    نیز هست. زیرا فرستنده با استفاده از همان تابع کدگذاری پیام‌ها را ارسال می‌کند و گیرنده‌ی
    $i$
    با استفاده از تابع کدگشایی خود می‌تواند دقیقا پیام
    $i$ام را بازیابی کند. در واقع در
    \picod
    باید تمام حالت‌های ممکن برای بازیابی هر پیامی برای هر گیرنده را در نظر بگیریم و بین همه‌ی آن‌ها کد با کم‌ترین طول را انتخاب کنیم که
    \icod
    فقط یکی از آن حالت‌ها است.
\end{proof}


در ادامه روی کدهای خطی(\autoref{def:linearcode}) تمرکز می‌کنیم. هر زمانی که مقدار
$t$
صریحا بیان نشود برابر یک است. همان طور که در بخشی‌های پیش‌رو در بررسی الگوریتم‌ها خواهیم دید برای حالت
$t > 1$
تغییر کوچکی در الگوریتم‌های حالت
$t = 1$
می‌دهیم. در نتیجه در اکثر مواقع بررسی حالت
$t = 1$
در قضایا برای کفایت می‌کند. همچنین برای راحتی از میدان
$\mathbb{F}_2$
استفاده می‌کنیم. بسیاری از نتایج بدون هیچ تغییری برای هر میدانی برقرارند.

\section{\lpicod،
\nphard
است}
اکنون می‌خواهیم ثابت کنیم که
\transf{کدگذاری اندیس منعطف خطی}{Linear Pliable index coding, \lpicod}،
\nphard
است. برای این کار،
\lpicod
را به
\transf{سه-صدق پذیری همگن یک درست}{MONOTONE-1in3-SAT}
کاهش می‌دهیم.

\begin{definition}[سه-صدق پذیری همگن یک درست]
    برای یک مصداق از مسئله‌ی
    \transf{صدق پذیری}{3SAT}
    که تمام اتم‌ها بدون نقیض ظاهر شده‌اند، مسئله‌ی سه-صدق پذیری همگن یک درست این است که آیا یک مقداردهی به اتم‌ها وجود دارد به صورتی که عبارت را ارضا کند در حالی که در هر جمله (که تمام جملات حاوی سه اتم‌اند) دقیقا یک اتم صادق باشد.
\end{definition}

\namef{شیفر}{Schaefer}
در مقاله معروف خود
\cite{10.1145/800133.804350}
نشان می‌دهد که مسئله‌ی بالا،
\nphard
است.

فرض کنید
$\phi$
عبارتی با اتم‌های
$\alpha_1, \ldots, \alpha_M$
و
$N_0$
جمله به شکل روبرو باشد:
$$\phi(\alpha_1, \ldots, \alpha_M) = \bigwedge\limits_{i = 1}^{N_0} CL_i, CL_i = (\alpha_{i, 1} \vee \alpha_{i, 2} \vee \alpha_{i, 3})$$

لم زیر کاهش به مسئله‌ی
\picod
را نشان می‌دهد.
\begin{lemma}
    برای یک نمونه از مسئله‌ی سه-صدق پذیری همگن یک درست مانند
    $\phi$
    یک نمونه از مسئله‌ی
    \picod
    مانند
    $I_{\phi, M, N_0}$
    وجود دارد به گونه‌ای که
    $\phi$
    صدق پذیر است اگر و تنها اگر
    $I_{\phi, M, N_0}$
    کدی خطی با طول یک داشته باشد.
\end{lemma}
\begin{proof}
    برای
    $\phi$
    داده‌شده
    $I_{\phi, M, N_0}$
    را به این شکل می‌سازیم:
    \begin{enumerate}
        \item
        $N_0$
        گیرنده
        $c_i: i \in [N_0]$
        که
        $c_i$
        متناظر جمله‌ی
        $i$
        است.
        \item
        $M$
        متغیر تصادفی
        $b_i: i \in [M]$
        که
        $b_i$
        متناظر اتم‌
        $i$-ام است.
        \item
        اطلاعات جانبی گیرنده‌ی
        $i$
        به شکل زیر است:
        $$S_i = {j: \alpha_j \notin CL_i}$$
        یعنی هر گیرنده
        $M - 3$
        پیام را به صورت اطلاعات جانبی می‌داند.
    \end{enumerate}
    ($\Rightarrow$): فرض کنید کدی خطی با طول یک برای
    $I_{\phi, M, N_0}$
    وجود داشته باشد. پس پیامی که فرستنده ارسال می‌کند به شکل
    $Y = (Y_1) = (b_{\lambda_1} \oplus \ldots \oplus b_{\lambda_s})$. چون تمام گیرنده‌ها پیام جدیدی را با دریافت تنها یک پیام بازیابی کرده‌اند پس به ازای هر گیرنده‌ی $i$ در پیام ارسالی توسط فرستنده حداقل یکی از پیام‌هایی که که گیرنده‌ی $i$ در اطلاعات جانبی خود ندارد دخیل است. یعنی
    $\forall i \in [N_0]: \exists \lambda_k \in \lambda: \lambda_k \notin S_i$
    و چون باید بتوان با
    $Y_1$
    دریافت شده پیامی را بازیابی کرد باید دقیقا یک
    $\lambda_k$
    به ازای هر
    $i$
    وجود داشته باشد. حال اگر در
    $\phi$
    در جمله
    $i$ام اتم
    $\alpha_{\lambda_k}$
    را صادق قرار دهیم(تمام جملات که متناظر گیرنده‌ها هستند) صادق می‌شوند و عبارت ارضا می‌شود. همچنین در هر جمله دقیقا یک اتم صادق وجود دارد و در نتیجه مقداردهی معتبری خواهیم داشت. در نتیجه یک کد خطی با طول یک را می‌توان برای تولید یک مقداردهی صادقی برای
    $\phi$
    استفاده کرد.

    ($\Leftarrow$): دقیقا مشابه برهان قسمت قبلی می‌توان یک کد خطی با طول یک براساس یک مقداردهی صادق ساخت.
\end{proof}
چون سه-صدق پذیری همگن یک درست یک مسئله‌ی
\nphard
است، طبق لم قبلی پیدا کردن کد با کم‌ترین طول برای
\lpicod
نیز
\nphard
است.
\section{کران بالا برای حداقل تعداد ارسال مورد نیاز در
\picod}

\subsection{کران بالا}
در این بخش ثابت خواهیم کرد که برای
\picodt
کران بالا برای حداکثر تعداد ارسال مورد نیاز از مرتبه لگاریتمی نسبت به تعداد گیرنده‌ها وجود دارد. همچنین یک مثال سخت ارائه می‌دهیم که نشان می‌دهد حالت تساوی کران نیز رخ می‌دهد.

ایده‌ی کلی این است که یک زیرگروه از گیرنده‌ها را پیدا کنیم، با یک ارسال آن‌ها را ارضا کنیم و همین فرایند را با بقیه گیرنده‌ها تکرار کنیم. در
\autoref{lemma:pliable2015:3}
مجموعه‌ای از گیرنده‌ها که برای فرایند بالا مناسب‌اند را معرفی و ثابت می‌کنیم با یک ارسال ارضا می‌شوند. سپس در
\autoref{theorem:log1}
کران لگاریتمی را برای
\picodtt{1}
ثابت می‌کنیم. سپس با تعمیم این ایده‌ها حالت
\picodtt{t}
را ثابت می‌کنیم.

ابتدا به تعاریف زیر نیاز داریم:
\begin{definition}[همسایه‌های
$i$-نامتصل]
    \label{def:nonnei}
    برای مجموعه‌ی
    $B_1 \subseteq B$
    مجموعه‌ی
    $W_i(B_1) \subseteq C$
    تمام همسایه‌های
    $B_1$اند
    $C$
    که به
    $\card{B_1} - i$
    رأس از رأس‌های
    $B_1$
    وصل‌اند. به
    $W_i(B_1)$
    مجموعه‌ی همسایه‌های
    $i$-نامتصل
    $B_1$
    می‌گوییم.

    همچنین برای هر مجموعه‌ رأس
    $B_1$
    تعریف می‌کنیم
    $t(B_1) = \max\set{i: W_i(B_1) \neq \emptyset }$
\end{definition}

\begin{lemma}
    \label{lemma:pliable2015:3}
    اگر
    $C' \subseteq C$
    مجموعه‌ای شامل
    $k$
    گیرنده‌ باشد، بدون از دست رفتن کلیت فرض می‌کنیم
    $C' = \set{c_1, \ldots, c_k}$
    و تعریف می‌کنیم:
    $\dmax = \max\set{R_i: i \in [k] }$
    و
    $\dmin = \min\set{R_i: i \in [k] }$. همچنین فرض می‌کنیم که هر گیرنده حداقل به دنبال یک پیام هست یعنی
    $\dmin \geq 1$. اگر ثابت
    $r \geq 1$
    وجود داشته باشد که
    $\dmax \leq r \dmin$، در این صورت کدی با اندازه‌ی
    $O(\log(k))$
    وجود دارد که تمام اعضای
    $C'$
    را ارضا کند.
\end{lemma}
\begin{proof}
    برای نشان دادن حکم از یک اثبات احتمالاتی استفاده می‌کنیم.

    فرض کنید که
    $B_0$
    پیام‌هایی باشند که اطلاعات جانبی اعضای
    $C$
    نیستند یعنی
    $B_0 = \cup_{i = 1}^{k} R_i$. اگر
    $\dmax = 1$
    باشد در این صورت ارسال
    $Y_1 = \sum_{b \in B_0} b$
    تمام گیرنده‌های
    $C'$
    را ارضا می‌کند که کدی با طول یک به ما می‌دهد. اگر
    $\dmax \geq 2$
    بود زیرمجموعه‌ی
    $B_1 \subseteq B_0$
    را به صورت زیر می‌سازیم. به صورت تصادفی هر عضو
    $B_0$
    را با احتمال
    $p$
    در
    $B_1$
    قرار می‌دهیم.

    احتمال این‌که گیرنده‌ی
    $c_i$
    به دقیقا یکی از اعضای
    $B_1$
    وصل نباشد برابر است با
    $$P_i = R(c_i) p (1 - p)^{R(c_i) - 1}$$
    توجه کنید که رأس
    $c_i$
    به تمام رأس‌های
    $B_0 \setminus R_i$
    وصل است. همچنین
    $R_i \subseteq B_1$، پس تنها باید یک رأس از
    $R_i$
    انتخاب شود. در نتیجه امید ریاضی اندازه‌ی
    $w_1(B_1)$
    برابر است با
    \begin{equation}
        \mathbb{E}[|W_1(B_1)] \geq k \dfrac{\dmin}{max}(1 - \dfrac{1}{\dmax})^{\dmax - 1} \geq k \dfrac{\dmin}{max} \geq \dfrac{k}{\gamma r}
    \end{equation}
    که منظور از
    $\gamma$
    \transf{ثابت اویلر}{Euler's constant, Euler–Mascheroni constant}
    است. چون حتما یک مجموعه‌ی
    $B_1$
    وجود دارد که از
    $\card{W_1(B_1)}$
    از امیدریاضی بیشتر مساوی باشد. در نتیجه ارسال جمع اعضای
    $B_1$
    کسر ثابتی
    $k$
    گیرنده‌ را ارضا می‌کند و حداکثر
    $k(1 - \dfrac{1}{\gamma r})$
    گیرنده می‌ماند.

    حال اگر گیرنده‌های ارضا شده را از گراف حذف کنیم چون رأس‌های متناظر با پیام‌ها تغییری نمی‌کنند پس همچنان در گراف باقی‌مانده
    $\dmax \leq r \dmin$
    . در نتیجه عمل بالا را باز هم می‌توانیم تکرار کنیم تا زمانی که تعداد ثابتی از گیرنده‌ها باقی بماند(مثلا یک گیرنده). برای هر گیرنده‌ی باقی‌مانده یکی از پیام‌هایی که ندارد را ارسال می‌کنیم. چون در هر گام تعداد گیرنده‌های باقی‌مانده با یک ضریب کم می‌شود حداکثر در
    $O(\log(k))$
    مرحله این تکرار تمام می‌شود و کدی با طول
    $O(\log(k))$
    به دست می‌آید.
\end{proof}

\begin{remark}
    در عمل اگر
    $\exists s: \forall i \in [n]: S_i = s$
    در این صورت کدی با طول
    $O(\log(n))$
    وجود دارد که تمام گیرنده‌ها را ارضا می‌کند. این یک بهبود توانی نسبت به
    $\Omega(n)$
    در
    \icod
    در بدترین حالت برای
    $m = n$
    است.
\end{remark}

حال با استفاده از لم قبلی می‌توانیم با تقسیم گیرنده‌ها قضیه زیر را ثابت کنیم:
\begin{theorem}
    \label{theorem:log1}
    برای هر نمونه از
    \picodtt{1}
    با
    $n$
    گیرنده و
    $m$
    پیام کدی با طول
    $$O(\min\set{m, n, \log(m)(1 + \log^{+}(\dfrac{n}{\log(m)}) ))})$$
    وجود دارد.
\end{theorem}
\begin{proof}
    درجه رأس‌های
    $c_i$
    گراف می‌تواند
    $0$
    تا
    $m - 1$
    باشد. این رئوس را به دسته‌های زیر تقسیم می‌کنیم:
    $S_{i}' = \set{c_l: m - 2^{i - 1} \geq d(c_l) > m - 2^i}$
    برای راحتی مجموعه‌های تهی را دور می‌ریزیم و بقیه را
    $S_1, \ldots, S_g$
    نام‌گذاری می‌کنیم. به وضوح شرط لم قبلی در تمام این مجموعه‌ها با
    $r = 2$
    برقرار است و همچنین
    $g \leq 1 + \ceil{\log_2(m)}$. حال این مجموعه‌ها را به دو گروه تقسیم می‌کنیم.
    $G_1 = \set{ S_i: \card{S_i} < 3}$
    و
    $G_2 = \set{ S_i: \card{S_i} \geq 3}$
    چون
    $\sum_{i \in G_2} \card{S_i} \leq \sum_{i \in [g]} \card{S_i} = n$
    پس
    $\card{G_2} \leq \dfrac{n}{3}$. از طرفی
    $\card{G_1} + \card{G_2} = g$. حال طبق لم قبلی به ازای یک ثابت
    $K_1$
    به
    $K_1(1 + \log(\card{S_i}))$
    پیام برای ارضا اعضای
    $S_i$
    نیاز داریم که یک اضافی در جمله داخل پرانتز برای حالتی است که
    $\card{S_i} = 1$
    . در نتیجه در نهایت طول کد برابر می‌شود با:
    \begin{align}
        K_1 \sum\limits_{i = 1}^{g} (1 + \log(\card{S_i}))
        & \leq K_1 \sum_{i \in G_1}    (1 + \log(\card{S_i})) + K_1 \sum_{i \in G_2} (1 + \log(\card{S_i})) \\
        & \leq K_1 \card{G_1} (1 + \log(2)) + K_1 \card{G_2} + K_1 \card{G_2} \log(\dfrac{\sum_{i\in G_2} \card{S_i}}{\card{G_2}}) \\
        & \leq q K_1 \card{G_1} (1 + \log(2)) + K_1 \card{G_2} + K_1 \card{G_2} \log(\dfrac{n}{\card{G_2}}) \\
        & = O(\log m (1 + \log^{+}(\dfrac{n}{\log m})))
    \end{align}
    نامساوی دوم از
    \hyperref[Jensen]{
        نامساوی ینسن
    }
    روی تابع لگاریتم و این نکته که
    $\forall i \in G1: \card{S_i} < 3$
    نتیجه می‌شود. آخرین خط این گونه توجیه می‌شود که تابع
    $x \log(n/x)$
    روی
    $x \leq n/e$
    ناکاهشی است در نتیجه اگر
    $1 + \ceil{ \log m} \leq n/3$
    باشد چون
    $\card{G_2} \leq g$
    داریم
    $$\card{G_2} \log (\dfrac{n}{\card{G_2}}) = O(\log m \log (\dfrac{n}{\log m}))$$
    و اگر برعکس این حالت برقرار باشد یعنی
    $1 + \ceil{ \log m } \ge n/3$
    چون
    $\card{G_2} \leq n/3$
    داریم:
    $$\card{G_2} \log(\dfrac{n}{\card{G_2}}) = \dfrac{n}{3} \log(3) = O(\log m)$$

    همچنین اگر تمام پیام‌ها را ارسال کنیم($m$
    ارسال) تمام گیرنده‌ها می‌توانند پیام جدیدی را بازیابی کنند. اگر به ازای هر گیرنده یک پیام خاص را ارسال کنیم باز هم این برقرار است. در نتیجه با کمینه گرفتن از این سه نتیجه، قضیه اثبات می‌شود.
\end{proof}
\begin{corollary}
    در عمل اگر تعداد پیام‌ها، چندجمله‌ای بر اساس تعداد گیرنده‌ها باشد یعنی
    $m = O(n^{\delta})$
    برای یک ثابت
    $\delta > 0$
    ، آنگاه یک کد با سایز
    $O(\log^2 n)$
    برای هر نمونه از
    \picodtt{1}
    وجود دارد.
\end{corollary}

حال نتیجه‌ی بالا را برای
\picodtt{t}
وقتی
$t > 1$
است تعمیم می‌دهیم. برای یک مجموعه
$B_1$
از پیام‌ها مجموعه‌ی همسایه‌های
$i$- نامتصل آن را در نظر بگیرید. طبق بحث‌های بالا وقتی
$i = 1$
است، یعنی برای
$W_1(B_1)$
تنها با یک ارسال اعضا ارضا می‌شوند. با به کار بردن
\autoref{lemma:pliable20151}
نتیجه می‌شود که برای ارضای اعضای
$W_2(B_1)$
نیز تنها دو ارسال لازم است.

%\begin{example}
% shit
%	 In Fig. 3a, for B = {b1, b2}, W1(B) =	{c1, c2, c4, c5} and W2(B) = {c3}. As previously mentioned, a	single linear combination is sufficient to satisfy all the client	vertices in W1(B). Applying Lemma 1, two independent linear	combinations of messages in B are sufficient to satisfy all	client vertices in W2(B).	In another example, consider the graph in Fig. 3b. In this	case B = {b1, b2, b3}, W1(B) = {c1, c2, c4} and W2(B) =	{c3, c5}. If b1 + b2 + b3 is sent to the 3 vertices in W1(B),	each of them can decode a message it does not have. For	the two vertices in W2(B), notice that they are adjacent to	two different subsets of message vertices. In this case, we can	apply Lemma 1 to obtain an additional linear combination of	{b1, b2, b3} such that each of c3, c5 can decode two messages	it does not have.
%\end{example}
با تکرار اثباتی کاملا مشابه
\autoref{lemma:pliable20151}، لم زیر که تعمیمی از آن است اثبات می‌شود.
\begin{lemma}
    برای هر
    $B_1 \subseteq B$
    ،
    $t(B_1)$
    ترکیب خطی از اعضای
    $B_1$
    وجود دارد که هر گیرنده‌ی
    $W_i(B_1)$
    می‌تواند
    $i$
    پیام جدید را بازیابی کند برای
    $i \in [t(B_1)]$
\end{lemma}
برای ساخت کدهای کوتاه تر برای
\picodtt{t}
یک زیر مجموعه‌ی مناسب از پیام‌ها را انتخاب می‌کنیم که تعداد زیادی از گیرنده‌ها، تقریباً
$t$
تای آن‌ها را به عنوان اطلاعات جانبی نداشته باشند. برای این کار از روش‌های احتمالاتی استفاده می‌کنیم. مشابه
\autoref{lemma:pliable2015:3}، لم زیر یک کران بالا برای وقتی که اندازه‌ی مجموعه‌ی اطلاعات جانبی گیرنده‌ها واریانس کمی دارد به ما می‌دهد.
\begin{lemma}
    \label{lemma:pliable2015:6}
    اگر
    $C' \subseteq C$
    مجموعه‌ای شامل
    $k$
    تا از گیرنده‌‌ها باشد. بدون از دست رفتن کلیت فرض می‌کنیم
    $C' = \set{c_1, \ldots, c_k}$
    . و تعریف می‌کنیم:
    $\dmax = \max\set{R_i: i \in [k] }$
    و
    $\dmin = \min\set{R_i: i \in [k] }$
    و همچنین فرض می‌کنیم که هر گیرنده حداقل به دنبال
    $t$
    پیام هست یعنی
    $\dmin \geq t$
    . اگر ثابت
    $r \geq 1$
    وجود داشته باشد که
    $\dmax \leq r \dmin$
    در این صورت کدی با اندازه‌ی
    $O(\min\set{t \log(k), t + \log^2 k})$
    وجود دارد که تمام اعضای
    $C'$
    بتوانند
    $t$
    پیام جدید را بازیابی کنند.
\end{lemma}
\begin{proof}
    اگر
    $k = 1$
    باشد در این صورت تنها گیرنده
    $C'$
    با ارسال
    $t$
    پیام متفاوت که جز اطلاعات جانبی‌اش نیست ارضا می‌شود.
    $B_0 = \cup_{i = 1}^{k} R_i$

    وقتی
    $k \geq 2$
    به صورت تصادفی زیرمجموعه‌ی
    $B_1 \subseteq B_0$
    را به این صورت انتخاب می‌کنیم که هر رأس با احتمال
    $p = \dfrac{t}{\dmax}$
    انتخاب می‌شود. بدون از دست رفتن کلیت گیرنده‌ی
    $c_1$
    با درجه‌ی
    $d = \card{B_0} - R_i$
    را در نظر بگیرید.
    $Z_i$
    متغیر تصادفی شاخصی است که یک است اگر
    $i$-امین همسایه‌ی
    $c_1$
    در
    $B_1$
    حضور داشته باشد. واضح است که
    $Z_i$
    ها متغیرهای تصادفی برنولی مستقل با توزیع یکسان با
    $P(Z_i = 1) = p$
    هستند. قرار دهید
    $Z = Z_1 \ldots + Z_d$
    حال داریم:
    \begin{equation}
        \mathbb{E}[Z] = \mathbb{E}[\sum\limits_{i = 1}^{d} X_i ] = \sum\limits_{i = 1}^{d} \mathbb{E}[X_i] = dp = \dfrac{dt}{d_{max}}
    \end{equation}
    چون
    $d \leq \dmax \leq rd$
    و همچنین
    $t \geq \mathbb{E}[Z] \geq \dfrac{t}{r}$. حال به کرانی برای میزان تمرکز
    $\mathbb{E}[Z]$
    نیاز داریم. چون
    $Z$
    جمع چند متغیر برنولی مستقل با توزیع یکسان است می‌توان از نامساوی‌های چرنوف زیر استفاده کرد
    \cite{Dubhashi_Panconesi_2009}
    برای هر
    $\epsilon \in (0, 1)$
    داریم
    $$P(Z < (1 - \epsilon) \mathbb{E}[Z]) \leq e^{- \dfrac{\epsilon^2}{2} \mathbb{E}[Z]}$$
    و
    $$P(Z > (1 + \epsilon) \mathbb{E}[Z]) \leq e^{- \dfrac{\epsilon^2}{3} \mathbb{E}[Z]}$$
    فرض کنید
    $t \geq 24 r \log k$
    . اگر
    $\epsilon = \sqrt{\dfrac{6r \log k}{t}}$
    انتخاب کنیم خواهیم داشت
    $\epsilon \leq \dfrac{1}{2}$
    آنگاه با استفاده از این‌که
    $\mathbb{E}[Z] \geq t/r$
    داریم:
    \begin{equation}
        P(Z > 3 \mathbb{E}[Z]/x) \leq P(X > (1 + \epsilon) \mathbb{E}[Z]) \leq e^{-\dfrac{\epsilon^2}{3} \mathbb{E}[Z]} \leq e^{- \dfrac{6 r \log k}{3 t} \cdot \dfrac{t}{r}} = e^{- 2 \log k} = k^{-2}
    \end{equation}
    به طور مشابه داریم:
    \begin{equation}
        P(Z < \mathbb{E}[Z]/x) \leq P(X > (1 -- \epsilon) \mathbb{E}[Z]) \leq e^{-\dfrac{\epsilon^2}{2} \mathbb{E}[Z]} \leq e^{- \dfrac{6 r \log k}{2 t} \cdot \dfrac{t}{r}} = e^{-3 \log k} = k^{-3}
    \end{equation}

    با ترکیب دو کران بابا خواهیم داشت:
    \begin{equation}
        P(\dfrac{\mathbb{E}[Z]}{2} \leq Z \leq \dfrac{3\mathbb{E}[Z]}{2}) \geq 1 - \dfrac{1}{k^2} - \dfrac{1}{k^3}
    \end{equation}
    که نتیجه می‌دهد با احتمال ناصفر رأس گیرنده‌ای هست که به، بین
    $\mathbb{E}[Z]/2$
    و
    $3\mathbb{E}[Z]/2$
    از پیام‌های
    $B_1$
    وصل نیست. امید ریاضی تعداد گیرنده‌هایی که این ویژگی را داشته باشند حداقل
    $k - \dfrac{1}{k} - \dfrac{1}{k^2} \geq k - 3, \forall k \geq 2$
    است. در نتیجه حداقل یک زیرمجموعه‌ی
    $B_1$
    وجود دارد که امید ریاضی آن بیشتر مساوی باشد. پس زیر مجموعه‌ی
    $B_1$
    وجود دارد که تمام گیرنده‌ها بین
    $\mathbb{E}[Z] / 2$
    و
    $3\mathbb{E}[Z]/2$
    پیام نادانسته در آن دارند. توجه کنید که
    $\mathbb{E}[Z]$
    برای هر گیرنده متفاوت است و اینجا به دلیل واضح بودن مسئله دقتمان را کم کردیم و
    \transf{سوءاستفاده از نمادگذاری}{abuse of notation}
    رخ داد. اگر از
    \autoref{lemma:pliable20151}
    استفاده کنیم، مجموعه‌ای وجود دارد با حداکثر
    $3\mathbb{E}[Z]/2 \leq 3t/2$
    ترکیب خطی از پیام‌ها که هر گیرنده می‌تواند حداقل
    $\mathbb{E}[X]/2 \geq t/2r$
    پیام جدید را بازیابی کند. توجه کنید که اگر
    $3t/2 > \dmax$
    آنگاه تعداد ترکیب‌های خطی حداکثر
    $\dmax$
    خواهد بود.

    در نتیجه اگر
    $t \geq 25 r \log k$
    با استفاده از
    $O(t)$
    ارسال می‌توان مطمئن شد که همه‌ی گیرنده‌ها حداقل
    $t/2r$
    پیام جدید را بازیابی می‌کنند. حال می‌توانیم به صورت بازگشتی همین کار را تکرار کنیم تا زمانی که به نمونه‌ای از مسئله برسیم که هر گیرنده حداثکر
    $t' = t(1 - 1/2r)$
    پیام جدید نیاز داشته باشد. اما در نمونه‌ی جدید مسئله،
    $\dmin$
    و
    $\dmax$
    تغییر کرده‌اند و ممکن است نسبتشان از یک ثابت بیشتر شده باشد. حالتی را در نظر بگیرید که
    $\dmin \geq 2t$
    با هر ارسال بعد از این‌که گیرنده‌ها پیامی بازیابی می‌کنند درجه رأس‌های گیرنده‌ها بیشتر می‌شود. برای
    $0 \leq t_1, t_2 \leq t$
    نسبت
    $\dmax$
    و
    $\dmin$
    به صورت زیر محدود می‌شود:
    \begin{equation}
        \dfrac{\dmax - t_1}{\dmin - t_2} \leq r + \dfrac{rt_2 - t_1}{\dmin - t_2} \leq 2r
    \end{equation}
    که نامساوی آخر از
    $t_2 \leq t$
    و
    $\dmin \geq 2t$
    نتیجه می‌شود. پس در این حالت نسبت، از یک ثابت
    $2r$
    در بدترین حالت بیشتر نمی‌شود و می‌توان روند بازگشتی را ادامه داد.

    حال حالت
    $\dmin < 2t$
    را در نظر بگیرید. چون رأس‌های گیرنده‌ها حداکثر به
    $d_{max} \leq 2tr$
    رأس پیام وصل نیستند، تمام آن‌ها با ارسال
    $O(t)$
    ترکیب خطی بدون نیاز به روند بازگشتی قبلی ارضا می‌شوند.

    روند بازگشتی را تا زمانی که
    $t' < 24r \log k$
    شود ادامه می‌دهیم. سپس با
    $t'$
    بار استفاده از
    \autoref{lemma:pliable2015:3}
    باقی‌مانده گیرنده‌ها نیز با
    $O(t' \log k)$
    ارسال ارضا می‌شوند. در نتیجه اگر
    $f(k, t)$
    را تعداد پیام‌های لازم برای این‌که هر کدام از
    $k$
    گیرنده
    $t$
    پیام جدید بازیابی کند بگیریم به تابع بازگشتی زیر می‌رسیم:
    \begin{equation}
        f(k, t) \leq \begin{cases}
                         f\left(k, t\left(1-\frac{1}{2 r}\right)\right)+O(t) & t \geq 24 r \log k \\ O(t \log k) & \text { \# }
        \end{cases}
    \end{equation}
    با حل این تابع بازگشتی به جواب
    $f(k, t) = O(\min\set{t \log k, t + \log^2 k})$
    میرسیم.
\end{proof}

\begin{theorem}
    \cite{pliable2015paper}
    \label{thm:pliable2015_7}
    برای هر نمونه از
    \picodtt{t}
    با
    $m$
    پیام و
    $n$
    گیرنده، همه‌ی گیرنده‌ها را با کدی به طول زیر میتوان ارضا کرد:
    \begin{equation}
        O\left( \min\set{ t \log(m) (1 + \log^{+}(\dfrac{n}{\log m})), t \log(m) + \log(m) \log^2(n), m, n, tn}\right)
    \end{equation}
\end{theorem}
\begin{proof}
    این قضیه حالت
    $t$
    پیامه‌ی
    \autoref{theorem:log1}
    است، در نتیجه اثبات کاملا مشابهی دارد.

    درجه‌ی رأس‌های گیرنده در بازه‌ی
    $[1, m]$
    قرار می‌گیرد. رأس‌های گیرنده را در زیرمجموعه‌های
    $S_i^{'}$
    تقسیم می‌کنیم به صورتی که
    $S_i^{'} = \set{c_l | 2^{i - 1} \leq d(c_l) < 2^i }$. مانند قضیه تک پیامه، تنها مجموعه‌های ناتهی را در نظر می‌گیریم و با
    $S_1, \ldots, S_g$
    نشان می‌دهیم. واضح است که نسبت بیشینه درجه به کمینه درجه در هر مجموعه
    $S_i$
    حداکثر
    $r = 2$
    است و
    $g \leq 1 + \ceil{\log_2(m)}$. طبق
    \autoref{lemma:pliable2015:6}
    برای ارضای رأس‌های مجموعه‌
    $S_i$
    به حداکثر
    $\min (K_1 t (1 + \log(\card{S_i})), K_2(t + \log^2(\card{S_i})))$
    پیام نیاز داریم(به ازای دو ثابت
    $K_1$
    و
    $K_2$
    ). با استفاده از جمله اول و اثبات
    \autoref{theorem:log1}
    تعداد پیام‌ها در مجموع با کران بالای زیر محدودی می‌شود:
    $$
    K_1 t \sum_{i=1}^g\left(1+\log \left(\card{S_i}\right)\right)=O\left(t \log m\left(1+\log ^{+}\left(\frac{n}{\log m}\right)\right)\right)
    $$
    همچنین اگر از جمله دوم استفاده کنیم به کران بالای زیر می‌رسیم:
    $$
    \begin{aligned}
        & K_2 \sum_{i=1}^g\left(t+\log ^2\left(\card{S_i}\right)\right) \\
        & \quad=K_2\left(t g+\sum_{i=1}^g \log ^2\left(\card{S_i}\right)\right) \leq K_2\left(t g+g \log ^2 n\right) \\
        & \quad=O\left(t \log m+\log m \log ^2 n\right)
    \end{aligned}
    $$
    همچنین با ارسال تمام پیام‌ها کران $m$ یا $t$ پیام به ازای هر گیرنده کران $tn$ را خواهیم داشت. که کمینه گرفتن بین تمام این کران‌ها قضیه را ثابت می‌کند.
\end{proof}

\begin{theorem}
    \cite{pliable2015paper}
    \label{thm:pliable2015_8}
    برای
    $m$
    به اندازه‌ی کافی بزرگ، اگر هر پیام با احتمال ثابت و مشخص
    $q$
    در اطلاعات جانبی یک گیرنده ضاهر شود، در آن صورت هر نمونه از
    \picodtt{t}
    را با کدی به طول
    $O(\min(t \log(n), t + \log^2(n)))$
    می‌توان حل کرد.
\end{theorem}
\begin{proof}
    طبق قانون اعداد بزرگ، درجه رأس هر گیرنده در یک گراف دوبخشی تصادفی حول
    $m (1 - q)$
    متمرکز است. برای هر
    $\epsilon > 0$
    برای
    $m$    های به اندازه‌ی کافی بزرگ با احتمال بالایی داریم:
    $d(c_i) \in [m(1 - q - \epsilon), m(1 - q + \epsilon)]$. اگر
    $\epsilon < q/3$
    انتخاب کنیم، نسبت بیشینه درجه به کمینه درجه کمتر مساوی دو خواهد بود. در نتیجه شرایط
    \autoref{lemma:pliable2015:6}
    در این جا صدق می‌کند و اثبات کامل می‌شود.
\end{proof}
\subsection{
    مثالی از
    \lpicod
    با حداقل طول کد لگاریتمی
}
در لم زیر که در
\cite{song2016deterministic}
آمده است، یک معیار جبری برای کدهای منعطف ارائه می‌دهیم. نسخه‌ی
$t$
پیامه این لم در
\cite{linqiphd}
آمده است. با استفاده از این لم نشان خواهیم داد که نمونه‌ای وجود دارد که کران پایین طول کد آن، نسبت به تعداد گیرنده‌ها لگاریتمی است.
\begin{lemma}
    \label{lemma:algebraicconstraint}
    در یک نمونه‌ی
    \lpicod
    که از ماتریس
    $A$
    با ستون‌های
    $\alpha_1, \ldots, \alpha_m$
    برای کدگذاری استفاده می‌شود، یعنی
    $Y = A b$
    توسط فرستنده ارسال می‌شود، اگر برای گیرنده‌ی
    $j$-ام داشته باشیم
    $i \in R_j$
    آنگاه شرط لازم و کافی برای اینکه گیرنده‌‌ی
    $c_j$
    بتواند پیام
    $b_i$
    را بازیابی کند، این است که ستون
    $\alpha_i$
    از ماتریس
    $A$
    وجود داشته باشد به صورتی که
    $\alpha_i \notin \sspan \set{A_{R_j} \setminus i}$.
\end{lemma}
\begin{proof}
    اگر شرط
    $\alpha_i \notin \sspan\set{A_{R_j} \setminus i}$
    برقرار باشد آنگاه برای هر بردار که در فضای پوچ
    $A_{R_j}$
    باشد یعنی
    $z \in \mathcal{N}(A_{R_j})$
    خواهیم داشت:
    $z_i = 0$
    زیرا طبق فرض اول مستقل خطی بودن ستون
    $0 = A_{R_j} z  = \alpha_1 z_1 + \ldots + \alpha_m z_m \Rightarrow z_i = 0$
    . از طرفی می‌دانیم که هر جواب یک معادله‌ی خطی را می‌توان به صورت جمع یک جواب خصوصی به علاوه‌ی یک بردار از مجموعه‌ی پوچ ماتریس نوشت. در نتیجه یک جواب برای
    $b_i$
    در معادله‌ی
    $Y' = A_{R_j} b_{R_j}$
    تنها جواب آن خواهد بود و در نتیجه گیرنده‌ی
    $c_j$
    می‌تواند به صورت یکتا
    $b_i$
    را بازیابی کند.

    برای طرف دیگر قضیه اگر
    $c_j$
    به صورت یکتا
    $b_i$
    را بازیابی کند ولی
    $\alpha_i \in \sspan\set{A_{R_j} \setminus \set{i}}$
    باشد، در این صورت جواب با درایه‌ی
    $z_i \neq 0$
    برای
    $0 = A_{R_j} z$
    خواهیم داشت. اگر یک جواب خصوصی
    $Y' = A_{R_j} b_{R_j}$
    را با این بردار از فضای پوچ جمع کنیم از طرفی باید باز هم جوابی برای معادله باشد و از طرفی مقدار
    $b_i$
    آن متفاوت با جواب خصوصی اصلی خواهد بود که یعنی
    $c_j$
    نمی‌تواند به صورت یکتا
    $b_i$
    را بازیابی کند.
\end{proof}
\begin{definition}[
    \picod
    کامل
]
    به یک نمونه از
    \picod
    که به ازای هر حالت ممکن از اطلاعات جانبی یک گیرنده وجود داشته باشد گفته می‌شود. در این نمونه به ازای
    $n$
    گیرنده
    $\log(n)$
    پیام داریم. برای مثال
    \autoref{figure:complete_instance}
    یک
    \picod
    کامل برای
    $n= 8$
    است.
\end{definition}
\begin{figure}
    \begin{center}
        \begin{tikzpicture}[->, >=stealth, auto, semithick, edge_style/.style={color=red, dashed}]
            % Set the positions of the nodes
            \node[circle, draw=blue, fill=blue!20, inner sep=0pt] (b1) at (1.5,2.5) {$b_1$};
            \node[circle, draw=blue, fill=blue!20, inner sep=0pt] (b2) at (3.5,2.5) {$b_2$};
            \node[circle, draw=blue, fill=blue!20, inner sep=0pt] (b3) at (5.5,2.5) {$b_3$};

            \node[circle, draw=green, fill=green!20, inner sep=0pt] (c0) at (-1,0) {$c_0$};
            \node[circle, draw=green, fill=green!20, inner sep=0pt] (c1) at (0,0) {$c_1$};
            \node[circle, draw=green, fill=green!20, inner sep=0pt] (c2) at (1,0) {$c_2$};
            \node[circle, draw=green, fill=green!20, inner sep=0pt] (c3) at (2,0) {$c_3$};
            \node[circle, draw=green, fill=green!20, inner sep=0pt] (c4) at (3,0) {$c_4$};
            \node[circle, draw=green, fill=green!20, inner sep=0pt] (c5) at (4,0) {$c_5$};
            \node[circle, draw=green, fill=green!20, inner sep=0pt] (c6) at (5,0) {$c_6$};
            \node[circle, draw=green, fill=green!20, inner sep=0pt] (c7) at (6,0) {$c_7$};
            % Draw the edges

            \draw[edge_style] (c1) -- (b1);
            \draw[edge_style] (c2) -- (b2);
            \draw[edge_style] (c3) -- (b3);
            \draw[edge_style] (c4) -- (b1);
            \draw[edge_style] (c4) -- (b2);
            \draw[edge_style] (c5) -- (b1);
            \draw[edge_style] (c5) -- (b3);
            \draw[edge_style] (c6) -- (b2);
            \draw[edge_style] (c6) -- (b3);
            \draw[edge_style] (c7) -- (b1);
            \draw[edge_style] (c7) -- (b2);
            \draw[edge_style] (c7) -- (b3);
            % Position the parts
        \end{tikzpicture}
    \end{center}
    \caption{
        \picod
        کامل برای
        $n = 8$
    }
    \label{figure:complete_instance}

\end{figure}
\begin{theorem}
    \label{thm:hardexam1}
    نمونه‌ای از
    \lpicod
    وجود دارد که به حداقل
    $\Omega(\log(n))$
    ارسال نیاز دارد.
\end{theorem}
\begin{proof}
    برای هر
    $n$
    یک
    \lpicod
    کامل با
    $n$
    گیرنده می‌سازیم. در یک
    \lpicod
    کامل فرستنده می‌تواند با ارسال
    $m = \log(n)$
    پیام همه‌ی گیرنده‌ها را ارضا کند(تمام پیام‌ها را ارسال کند.). حال کافی است نشان دهیم که بهتر از این کران وجود ندارد.

    با استقرا روی رتبه ماتریس کدگذاری
    $A$
    نشان می‌دهیم که رتبه ماتریس باید حداقل
    $m$
    باشد تا طبق
    \autoref{lemma:algebraicconstraint}
    گیرنده‌ها بتوانند ارضا شوند.

    فرض کنید
    $J \subseteq [m]$
    مجموعه‌ی اندیس تعدادی از ستون ها باشد. به ازای هر
    $J$
    یک گیرنده هست که
    $R_j = J$
    یعنی اندیس ستون‌های زیر ماتریس
    $A_{R_j}$
    که گیرنده‌ی
    $c_j$
    از آن برای بازیابی یک متغیر استفاده می‌کند برابر
    $J$
    باشد. در یک نمونه از
    \lpicod
    کامل، برای هر
    $J$
    ای
    \autoref{lemma:algebraicconstraint}
    باید برقرار باشد زیرا به ازای هر
    $b_i$
    یک گیرنده هست که دقیقا همان متغیر را باید بازیابی کند(
    گیرنده با اطلاعات جانبی
    $S_j = [m] \setminus \set{i}$
    ). برای
    $J = \set{j_1}$
    برای این‌که گیرنده‌ای که دقیقا باید همان متغیر را بازیابی کند ارضا شود ستون
    $j_1$
    نباید صفر باشد. پس:
    $\forall J, \card{J} = 1 \Rightarrow \rank(A_J) = 1$
    . برای
    $\card{J} = 2$
    هر دو ستون متفاوت ماتریس باید مستقل خطی باشند. در غیر این صورت اگر
    $J = \set{j_1, j_2}$
    مستقل خطی نباشند، در این صورت گیرنده‌
    $c_k$
    که
    $R_k = \set{j_1, j_2}$
    نمی‌تواند هیچ کدام از
    $b_{j_1}, b_{j_2}$
    را بازیابی کند. پس
    $\forall J, \card{J} = 2 \Rightarrow \rank(A_J) = 2$
    . حال برای فرض استفرا، فرض کنید
    $\forall J, \card{J} = l \Rightarrow \rank(A_J) = l$
    حال برای
    $\card{J} = l + 1$
    اگر تمام گیرنده‌هایی که
    $l + 1$
    پیام را ندارند یعنی
    $\card{R_k} = m - l - 1 $
    ارضا می‌شوند پس برای یک
    $i \in J$
    داریم
    $\alpha_i \notin \sspan(A_{J \setminus \set{i}})$
    پس
    $\rank(A_{J}) = \rank(\alpha_i) + \rank(A_{J \setminus \set{i}}) = 1 + l$
\end{proof}


\section{الگوریتم‌های تصادفی}

\subsection{الگوریتم \lr{GRCOV} }
\picod
ذات بسیار متفاوتی نسبت به
\icod
دارد. مثلا زمانی که یکی از متغیرها در مجموعه‌ی اطلاعات جانبی هیچ کدام از گیرنده‌ها نباشد تنها با یک ارسال می‌توان تمام گیرنده‌ها را ارضا کرد در حالی که در
\icod
این کد با طول یک، کد درستی نیست. در واقع بر خلاف
\icod
که هر تک گیرنده ممکن است به خاطر پیامی که می‌خواهد بازیابی کند طول کد را افزایش دهد در
\picod
گیرنده‌ها برهمکنش بیشتری روی طول کد دارند. به عنوان مثالی دیگر، زمانی که اندازه‌ی مجموعه‌ی اطلاعات جانبی تمام گیرنده‌ها برابر
$m - 1$
است، فرستنده کافی است مجموع تمام متغیرها را ارسال کند.

ایده‌ی اصلی الگوریتم‌های
\lr{GRCOV1 \& GRCOV2}
پیدا کردن مجموعه‌هایی از گیرنده‌ها است، که ویژگی‌های بالا را داشته باشند. این دو الگوریتم از ایده‌ی کلی یکسانی برخوردار هستند و تنها یک تفاوت کوچک دارد که در ادامه بیان خواهد شد.

\begin{remark}
    در مقاله
    \cite{pliablefirstpaper}
    نمادگذاری گراف اطلاعات جانبی بر خلاف نمادگذاری مرسوم این حوزه و این پایان‌نامه است. در واقع در مقالات بعدی این نمادگذاری تغییر کرد. در این مقاله اگر
    $j \notin S_i$
    آنگاه بین
    $b_j$
    و
    $c_i$
    یالی قرار دارد که برعکس نمادگذاری مرسوم است.
\end{remark}

فرض کنید برای دو متغیر
$b_{\sigma_1}$
و
$b_{\sigma_2}$
\footnote{
    همان طور که در
    \autoref{remark:xbdiff}
    گفته شد
    $b_{\sigma_1}$
    واقعا متغیر تصادفی نیست بلکه رأس متناظر متغیر
    $X_{\sigma_1}$
    است.
}
همسایه‌های این دو رأس
$N[b_{\sigma_1}] \cup N[b_{\sigma_2}]$
را بر اساس تعداد همسایه‌های خود در بخش
$B$
گراف به دو دسته تقسیم می‌کنیم. دسته اول رأس‌های
$c_i$ای که به همه به جز یکی وصل‌اند. در واقع به
$W_1(B_1)$
که در
\autoref{def:nonnei}
تعریف کردیم توجه می‌کنیم. حال اگر
$Y_1 = \bigoplus\limits_{b \in B_1} b$
را فرستنده ارسال کند تمام اعضای
$W_1(B_1)$
می‌توانند با اطلاعات جانبی خود، پیام جدیدی را با کم کردن اطلاعات جانبی خود از
$Y_1$
بازیابی کنند.

همان‌طور که پیشتر گفته شد، در الگوریتم
\GRCOVone
به دنبال مجموعه‌ای از رئوس گراف مانند
$B_1$
هستیم که
$\card{W_1(B_1)}$
بیشینه باشد. به جای این‌که در هر گام مجموعه‌ای با اندازه‌ی ماکسیمم پیدا کنیم به صورت حریصانه مجموعه‌ای با اندازه‌ی ماکسیمال پیدا می‌کنیم.

\begin{definition}[مجموعه‌ی ماکسیمال]
    به مجموعه‌ی
    $B_1 = \set{b_{v_1}, \ldots, b_{v_t}}$
    می‌گوییم ماکسیمال اگر برای هر رأس
    $b_{v_{t +1}} \notin B_1$
    داشته باشیم:
    \begin{equation}
        \tag{شرط ماکسیمال بودن}
        \card{W_1(B_1 \cup \set{b_{v_{t + 1}} })} < \card{W_1(B_1)}
    \end{equation}
\end{definition}

برای پیدا کردن یک مجموعه‌ی ماکسیمال با یک مجموعه‌ی تهی شروع می‌کنیم و در هر گام یک رأس جدید را به صورت حریصانه به طوری که
$\card{W_1(B_1)}$
را بیشینه کند انتخاب می‌کنیم. این کار را تا زمانی که نتوانیم ادامه بدهیم، انجام می‌دهیم.

زمانی که مجموعه‌ی ماکسیمال را پیدا کردیم، مطابق آنچه قبل‌تر گفته شد پیام
$Y_1 = \bigoplus\limits_{b \in B_1} b$
را به کد خود اضافه می‌کنیم(فرستنده این پیام را هم ارسال می‌کند). سپس تمام رأس‌های
$B_1$
و
$w_1(B_1)$
از گراف حذف می‌شوند و دوباره همین کار را تکرار می‌کنیم. در نهایت پس از
$s$
گام کد نهایی برابر
$Y = (Y_1, \ldots, Y_s), Y_i =  \bigoplus\limits_{b \in B_i} b$
خواهد بود. این الگوریتم
\GRCOVone\footnote{\lr{greedy cover}}
نام دارد. زمان اجرای این الگوریتم برابر
$O(mn^2)$
است.
\begin{algorithm}
    \caption[
        پوشش حریصانه
        \lr{GrCov}
    ]{
        پوشش حریصانه
        \lr{GrCov($G,m,n,t$) }
        \cite{pliable2015paper}}
    \label{algorithm:grcov}
    \begin{algorithmic}[1]
        \Require
        گراف اطلاعات جانبی
        $G$
        با
        $n$
        گیرنده و
        $m$
        پیام برای مسئله‌ی
        \picodtt{t}

        \Ensure
        یک اندیس‌کد منعطف برای
        $G$
        \State
        قرار دهید
        $\mathcal{C} = \set{}$
        \State برای هر
        $\forall i \in [n]$
        قرار دهید
        $CNT[i] = t$
        \While{
            $i$
            وجود دارد که
            $CNT[i] \neq 0$
        }
            \State
            قرار دهید
            $B \leftarrow \varnothing$.
            \While{$B$
                یک مجموعه‌ی ماکسیمال نیست}
                \State
                رأس
                $b_v \notin B$
                که
                $\card{W_1\left(B \cup\set{b_v}\right)}$
                بیشینه باشد را بیابید.
                \State قرار دهید
                $B \leftarrow B \cup\set{b_v}$
            \EndWhile
            \State قرار دهید
            $\mathcal{C} \leftarrow \mathcal{C} \cup\set{\sum_{u=1}^{\card{B}} b_{v_u}, b_{v_u} \in B}$
            \For{ $c_i \in W_1(B)$ }
                \If{
                    $c_i$
                    به
                    $b_j \in B$
                    متصل است
                }
                    \State یال متناظر در
                    $G$
                    را حذف کنید.
                \EndIf
                \State قرار دهید $CNT[i] \leftarrow CNT[i]-1$.
            \EndFor
        \EndWhile
        \State کد
        $\mathcal{C}$
        را به عنوان خروجی برگردانید.
    \end{algorithmic}
\end{algorithm}

الگوریتم
\lr{GRCOV2}
تغییر کوچکی نسبت به الگوریتم قبلی دارد. در الگوریتم قبلی وقتی یک مجموعه‌ی ماکسیمال را پیدا می‌کردیم هم
$B_1$
و هم متغیرهای مربوطه یعنی
$w_1(B_1)$
را حذف می‌کردیم. ولی ممکن است که این متغیرها در پیدا کردن مجموعه‌های ماکسیمال بهتری به ما کمک کنند. در این الگوریتم تنها
$B_1$
را حذف می‌کنیم.

زمان اجرای این الگوریتم نیز
$O(mn^2)$
است. شبه کد پیاده‌سازی اولیه از این الگوریتم در
\autoref{algorithm:grcov}
آمده است.

\subsection{الگوریتم \lr{SETCOV}}
فرگولی و براهما برای آزمایش کارایی الگوریتم‌های بالا، الگوریتم
\lr{SETCOV}
را معرفی می‌کند. این الگوریتم بر مبنای کاهش مسئله به
\icod
طراحی شده است. در
\picod
هر گیرنده
$i$
با بازیابی هر پیام از
$R_i$
ارضا می‌شود. برای تبدیل یک نمونه از
\picod
به
\icod
به ازای هر گیرنده
$R_i$
شبه‌گیرنده‌های
$c_{i, 1}, \ldots, c_{i, {R_i}}$
را قرار می‌دهیم که هر کدام به دنبال یکی پیام‌های
$R_i$
است و اطلاعات جانبی مشابهی مانند
$c_i$
دارد. در نتیجه
\icod
نهایی
$\sum\limits_{i = 1}^{n} \card{R_i}$
گیرنده خواهد داشت. این نمونه توسط هر کدام از الگوریتم‌های
\icod
قابل حل است. فرگولی از بین الگوریتم‌های ارائه شده در
\cite{25}
ساده‌ترین آن‌ها که بر مبنای
\transf{پوشش خوشه‌ای حریصانه}{greedy clique cover}
است را استفاده می‌کند.

پس از حل مسئله‌ی
\icod
ایجاد شده، فرض کنید
$Y$
کد خروجی الگوریتم پوشش خوشه‌ای حریصانه باشد. در این الگوریتم هر گیرنده تنها با استفاده از یکی از پیام‌ها، پیام مورد نظر خود را بازیابی می‌کند. در واقع هر پیام ارسال شده توسط فرستنده، تنها باعث می‌شود تعدادی از گیرنده‌ها توانایی بازیابی همان پیام به دست آورند. این مسئله باعث می‌شود به طور طبیعی یک رابطه‌ی "پوشش" دادن بین اعضای کد و گیرنده‌ها(در اینجا شبه‌گیرنده‌ها) ایجاد شود به این صورت که هر عضو کد، تعداد از گیرنده‌ها را پوشش می‌دهد. چون هدف اصلی ما گیرنده‌های
\picod
هستند و نه شبه‌گیرنده‌هایی که ساخته‌ایم، این مفهوم پوشش برای هر عضو کد را بر اساس گیرنده‌های \picod که ارضا می‌کند، تعریف می‌کنیم. یعنی:

\begin{definition}[پوشش پیام]
    اگر پیام
    $Y_t$
    شبه‌گیرنده‌های
    $$\set{c_{\sigma_1, \theta_{1, 1}}, c_{\sigma_1, \theta_{1, 2}},\ldots, c_{\sigma_1, \theta_{1, k_1}}} \cup \set{c_{\sigma_2, \theta_{2, 1}}, \ldots,} \cup \ldots \cup \set{c_{\sigma_s, \theta_{s, 1}}, \ldots, c_{\sigma_s, \theta_{s, k_s}}}$$
    را ارضا کند میگوییم $Y_i$ مجموعه‌ی
    $$C(Y_t) =\set{ c_{\sigma_1}, \ldots, c_{\sigma_s}}$$
    را پوشش می‌دهد.
\end{definition}
طبیعی است که هر گیرنده ممکن است در چندین مجموعه‌ی مختلف باشد. برای پیدا کردن یک جواب برای
\picod
تنها کافی است تعدادی از
$C(Y_i)$
ها را پیدا کنیم که کل گیرنده‌ها را پوشش دهند و در بین جواب‌ها دنبال جواب با کم‌ترین تعداد مجموعه‌‌ایم. این یک نمونه از مسئله‌ی معروف
\transf{پوشش مجموعه‌ای}{SET-COVER}
است که عناصر، گیرنده‌ها هستند و مجموعه‌ها پوشش‌های
$C(Y_i)$.

فرگولی در پیاده‌سازی آزمایشگاهی از روش استاندارد حریصانه تقریبی برای این مسئله استفاده می‌کند که زمان اجرای نهایی آن
$O(m^2 n^6)$
است.

\subsubsection{k-GRCOV}
برای حالت
\picodt
یک تعمیم طبیعی از الگوریتم قبلی وجود دارد. به جای این‌که در هر گام رأس‌هایی که یک پیام جدید را بازیابی می‌کنند حذف کنیم، هر رأس
$c_i$
یک شمارنده برابر
$\min(k, \card{R_i})$
دارد و در هر گام که یک رأس یک پیام جدید را بازیابی می‌کند این شمارنده را یک واحد کم می‌کنیم و رأس‌های
$w_1(B_j)$
را به رأس
$c_i$
به عنوان اطلاعات جانبی وصل می‌کنیم. هر رأسی که شمارنده‌اش به صفر برسد را هم حذف می‌کنیم. زمان اجرای این الگوریتم
$O(kmn^2)$
است.

\subsection{
    نتایج عددی
}
فرگولی و براهما الگوریتم‌های گفته شده را به صورت عملی روی نمونه‌های تصادفی از مسئله تست می‌کنند. در این آزمایش‌ها
$n = m = 500$
و هر یال در گراف اطلاعات جانبی را هم با احتمال
$p$
قرار می‌دهند. یعنی هر گیرنده با احتمال
$p$
آن پیام را از قبل می‌داند. همچنین برای مقایسه بهتر، در نمونه‌های تولیدی هر گیرنده می‌خواهد یک پیام متفاوت را در مسئله‌ی
\icod
بازیابی کند.
\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figs/ch3/pliable2012}
    \caption[
        طول کد بهینه
    ]{طول کد بهینه\cite{pliablefirstpaper}}
    \label{fig:pliable2012}
\end{figure}

در
\autoref{fig:pliable2012}
کاراریی الگوریتم‌های مختلف به ازای
$p$
های مختلف آمده است. برای هر
$p$
بیش از هزار بار آزمایش تکرار شده است. همان طور که در شکل مشخص است برای مسئله‌ی \picod تفاوتی چشمگیر بین الگوریتم‌های
\picod
و الگوریتم‌های مختلف مبتنی بر
\icod
وجود دارد (تفاوتی چشمگیری بین الگوریتم‌های مختلف
\icod
که در این مقایسه مهم باشد دیده نمی‌شود). در حالی که الگوریتم‌های گفته شده‌ی
\picod
به طور میانگین کمتر از
$11$
پیام ارسال می‌کنند الگوریتم‌های مبتنی بر
\icod
تنها وقتی
$p \geq 0.95$
است این کاراریی را دارند. یعنی زمانی که گراف اطلاعات جانبی بسیار \transf{
    متراکم
}{
    dense
} است.

در بین الگوریتم‌های بیان شده
\lr{GRCOV2}
بهترین عملکرد را دارد و
\lr{GRCOV1}
با فاصله‌ی کمی از آن عمل می‌کند.

الگوریتم
\lr{SETCOC}
روند جالبی دارد. در محدوده‌ی
$p \leq 0.5$
بهتر از
\lr{GRCOV2}
عمل می‌کند. بخشی از این عملکرد را می‌توان این گونه توجیه کرد که برای مقادیر کم
$p$
گراف اطلاعات جانبی تنک می‌شود و در نتیجه هر عضو کد(پیام ارسال شده توسط فرستنده)  برای تعداد بیشتری گیرنده حاوی اطلاعات(آنتروپی شرطی بر پایه اطلاعات جانبی هر گیرنده) است و در نتیجه وقتی به دنبال پوشش هستیم، هر مجموعه گیرنده‌های بیشتری را شامل می‌شود.

همان طور که در
\autoref{fig:pliable2012}
می‌بینید بیشینه‌ی طول کد در الگوریتم
\lr{GRCOV}
در آزمایش‌های انجام شده برای تمام نمونه‌های
\picod
کمتر از
$\log(500) \approx 6$
است. در شکل
\autoref{fig:pliable20122}
به ازای مقادیر محتلف
$n$
(که همچنان
$m = n$
) و نمونه‌های تصادفی یونیفرم می‌بینیم که برای
$n \geq 60$
طول کد حداکثر
$\log(n)$
است.

نتایج اجرای الگوریتم
\lr{k-GRCOV}
در
\autoref{fig:pliable20123}
آمده است. همان طور که مشخص است در عمل به طور میانگین با کدهای با طول کمتر از
$17$
می‌توان
$500$
گیرنده که به دنبال بازیابی
$5$
پیام هستند را ارضا کرد.
\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figs/ch3/pliable2012_2}
    \caption[حداکثر طول بهینه
    ]{
        حداکثر طول بهینه
        \cite{pliablefirstpaper}}
    \label{fig:pliable20122}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figs/ch3/pliable2012_3}
    \caption[
        طول کد برای
        $t$
        های مختلف
    ]{
        طول کد برای
        $t$
        های مختلف
        \cite{pliablefirstpaper}}
    \label{fig:pliable20123}
\end{figure}

\subsection{الگوریتم \lr{RANDCOV}}
این الگوریتم روند اثبات
\autoref{theorem:log1}
را برای پیدا کردن کد مناسب دنبال می‌کند. گیرنده‌ها به حداکثر
$g = O(\log(m))$
گروه ناتهی
$S_1, \ldots, S_g$
افراز می‌شوند به طوری که نسبت حداکثر و حداقل درجه در هر گروه حداکثر یک ثابت مشخص
$r$
باشد. فرض کنید بزرگترین درجه موثر در
$S_i$
برابر
$d_{max, i}$
باشد. در همسایگی
$N[S_i]$
هر رأس را با احتمال
$p_i = \dfrac{1}{d_{max, i}}$
انتخاب می‌کنیم. اگر
$B_i$
مجموعه‌ی رأس‌های انتخابی باشد، گیرنده‌های مجموعه‌ی
$W_1(B_i)$
ارضا می‌شوند. این گیرنده‌ها را حذف می‌کنیم و فرآیند را تا زمانی که تمام گیرنده‌های
$S_i$
ارضا شوند ادامه می‌دهیم. تعداد مجموعه‌های تصادفی انتخاب شده مورد نیاز برابر طول کد خواهد بود. این کار را برای تمام مجموعه‌های
$S_1, \ldots, S_g$
انجام می‌دهیم. پیاده سازی ساده‌ای از این الگوریتم زمان اجرای
$O(m n \log(n))$

\subsubsection{\lr{RANDCOV-PP}}
گرچه طول کد ساخته شده در الگوریتم
\lr{RANDCOV}
توسط
\autoref{theorem:log1}
محدود می‌شود، همان طور که در
\autoref{subsection:numerical}
خواهیم دید، یک پیاده سازی ساده بدون هیچ تغییری خروجی خوبی نسبت به
\lr{GRCOV}
نخواهد داشت. برای بهبود این الگوریتم می‌توانیم از مرحله‌ی پس‌پردازش زیر استفاده کنیم.

فرض کنید
$B_1$
و
$B_2$
دو مجموعه از گیرنده‌ها باشند. اگر هیچ یالی از
$B_1$
به
$W_1(B_2)$
نداشته باشد و همچنین هیچ یالی از
$B_2$
به
$W_1(B_1)$
وجود نداشته باشد آنگاه می‌توانیم با ارسال جمع تمام پیام‌های
$B_1 \cup B_2$
تمام گیرنده‌های
$W_1(B_1) \cup W_1(B_2)$
را ارضا کنیم. این ایده تنها به دو مجموعه وابسته نیست و می‌توان به هر تعداد مجموعه با شرایط گفته شده تعمیم داد. در هر مرحله به صورت حریصانه بیشترین تعداد مجموعه که در شرط بالا صدق می‌کنند را انتخاب می‌کنیم. با یک پیاده‌سازی ساده این الگوریتم را می‌توان همچنان در زمان
$O(m n \log(n))$
اجرا کرد.
\subsection{
    نتایج عددی
}
\label{subsection:numerical}
در
\autoref{fig:pliable2015}
نتایج شبیه‌سازی‌های الگوریتم‌های گفته شده را مشاهده می‌کنید. در این شبیه‌سازی‌ها تعداد گیرنده‌ها و پیام‌ها برابر
$n = m = 512$
است و میدان
$\mathbb{F}_2$
استفاده شده است. برای الگوریتم‌های
\lr{RANDCOV}
و
\lr{RANDCOV-PP}
نیز
$r = 3$
انتخاب شده است. برای ساخت هر نمونه از مسئله از روش زیر استفاده می‌کنیم.

هر گیرنده با احتمال
$p_{msg}$
هر کدام از پیام‌ها را ممکن است در اطلاعات جانبی خود داشته باشد. این مدل از ساخت نمونه می‌تواند مدلی برای
\lr{block-fading}
در
\transf{
    کانال‌های بی‌سیم
}{wireless channels}
باشد یعنی زمانی که نسبت
\transf{
    پیام به نویز
}{channel SNR =  channel signal to noise ratio}
کم است، مثلا لایه‌های بالایی شبکه با احتمال
$1 - p_{msg}$
پیام‌ها را پاک می‌کنند.
\autoref{fig:pliable2015a}
میانگین کارایی الگوریتم‌ها بر روی نمونه‌های مختلف را نشان می‌دهد. برای هر
$p_{msg}$
بیش از ده هزار نمونه ساخته شده است و طول کد میانگین روی تمام نمونه‌ها محاسبه شده است.

در بین الگوریتم‌های
\picods، الگوریتم
\lr{GRCOV}
بهترین کارایی را دارد. در واقع
\lr{GRCOV}
به صورت نظری روی نمونه‌های تصادفی، کارایی مشابهی به صورت
\transf{
    تقریبی
}{asymptotic}
با
\lr{RANDCOV}
دارد ولی در عمل کارایی بسیار بهتری دارد.

برای بررسی کارایی
\lr{GRCOV}
در حالت چند پیامه
\picodtt{t}
نمونه‌های تصادفی با
$p_{msg} = 0.5$
و مقادیر متفاوت
$t$
و
$n$
و
$m = n$
ساخته شده است. اگر در یک نمونه‌ تصادفی
$m - \card{S_i} < t$
در این صورت مطابق تعریف این گیرنده به دنبال بازیابی
$m - \card{S_i}$
پیام است. در
\autoref{fig:pliable2015b}
به ازای مقادیر مختلف
$t$
و چند مقدار مختلف
$n$
کارایی الگوریتم به تصویر کشیده شده است.

برای هر
$n$
بعد از کمی فاز ابتدایی، طول کد به صورت خطی نسبت به
$t$
افزایش می‌یابد. این نتیجه را به خاطر
\autoref{thm:pliable2015_8}
انتظار داشتیم. همچنین خط‌های نمودار در
\autoref{fig:pliable2015b}
برای
$t > 5$
به صورت تقریباً موازی هستند یعنی طول کد نسبت به
$n$
مستقل است. که با نتیجه‌ی
$O(t + \log^2(n))$
برای
$t$
های بزرگ مطابقت دارد. خط‌های نشان دهنده‌ی خطا(حداکثر و حداقل طول کد در نمونه‌ها) نیز مشابها روندی خطی را دنبال می‌کنند.

در
\autoref{fig:pliable2015c}
مطابق
\autoref{fig:pliable2015b}
است ولی این بار
$n$
در محور افقی آمده است و به ازای مقدارهای مختلف
$t$
نمودار رسم شده است.

در
\autoref{fig:pliable2015d}
تأثیر
$p_{msg}$
را بررسی می‌کنیم. این نمودار به ازای
$m = n = 100$
و مقادیر مختلف
$t$
رسم شده است.
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/ch3/pliable2015_a}
        \caption{}
        \label{fig:pliable2015a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/ch3/pliable2015_b}
        \caption{}
        \label{fig:pliable2015b}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/ch3/pliable2015_c}
        \caption{}
        \label{fig:pliable2015c}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/ch3/pliable2015_d}
        \caption{}
        \label{fig:pliable2015d}
    \end{subfigure}
    \caption[
        مقایسه‌ی الگوریتم‌های تصادفی
    ]{
        مقایسه‌ی الگوریتم‌های تصادفی
        \cite{pliable2015paper}
    }
    \label{fig:pliable2015}
\end{figure}

\section{
    الگوریتم قطعی
    \lr{BinGreedy}
}
\subsection{
    توصیف الگوریتم
}

در الگوریتم قطعی‌ای که معرفی خواهیم کرد از دو مفهوم گیرنده‌های موثر و درجه موثر استفاده می‌کنیم. برای یک پیام
$X_i$
(یا معادلا رأس
$b_j$
)
به گیرنده‌هایی که می‌خواهیم این پیام را بازیابی کنند، گیرنده‌های موثر می‌گوییم. همچنین درجه موثر یک پیام تعداد گیرنده‌هایی است که آن پیام دارد. با استفاده از این مفاهیم پیام‌ها را به گروه‌هایی تقسیم می‌کنیم که درجه موثر مشابهی دارند. سپس نشان می‌دهیم که به دلیل این ویژگی، در هر گروه با ارسال‌هایی که صرفاً بر اساس پیام‌های آن گروه است می‌توانیم ضریب ثابتی از گیرنده‌های موثر آن گروه را ارضا کنیم.

در هر گام الگوریتم
\lr{binary field greedy}
یا به اختصار
\lr{BinGreedy}
دو مرحله وجود دارد. مرحله مرتب سازی و مرحله‌ی ارسال حریصانه. در مرحله مرتب سازی، پیام‌ها را به صورت کاهشی بر اساس درجه‌ی موثر آن‌ها مرتب می‌کنیم سپس پیام‌ها را به
$\log(n)$
گروه افراز می‌کنیم. در مرحله‌ی ارسال، ترکیب خطی‌ای از پیام‌های هر گروه را در نظر گرفته و برای هر گروه دو ارسال انجام می‌دهیم. در نتیجه در کل
$2 \log(n)$
ارسال خواهیم داشت.

\begin{definition}
[گیرنده‌ی موثر و درجه موثر]
    برای یک جایگشت داده‌شده‌ی
    $\pi = (j_1, \ldots, j_m)$
    از پیام‌ها، درجه موثر
    $b_{j_l}$
    ناظر به جایگشت
    $\pi$
    را برابر تعداد همسایه‌های
    $b_{j_l}$
    که به هیچ یک از رأس‌‌های پیام ما قبل آن یعنی
    $b_{j_1}, \ldots, b_{j_{l - 1}}$
    متصل نیستند تعریف می‌کنیم. همچنین به گیرنده‌هایی که در درجه‌ی موثر رأس
    $b_{j_l}$
    شمرده شده‌اند، گیرنده‌های موثر
    $b_{j_l}$
    ناظر به جایگشت
    $\pi$
    می‌گوییم.
\end{definition}
\begin{notation}
    اگر همسایه‌های
    $b_j$
    را با
    $N[j]$
    نشان می‌دهیم همچنین تعریف می‌کنیم
    $N[j_1, \ldots, j_{l - 1}] = N[j_1] \cup \ldots \cup N[j_{l - 1}]$
    . رأس‌های موثر
    $b_{j_l}$
    را با
    $N^\dagger_\pi [j_l] = N[j_l] \setminus N[j_1, \ldots, j_{l - 1}]$
    نشان می‌دهیم همچنین درجه را با
    $d^\dagger_\pi[j_l] = \card{N^\dagger_\pi [j_l] }$
    نشان می‌دهیم.
\end{notation}


توجه کنید که درجه موثر و گیرنده‌های موثر یک رأس وابسته به جایگشت هستند.

\begin{example}
    در شکل
    \autoref{figure:example:effective}
    با جایگشت
    $\seq{b_1, b_2, b_3}$
    داریم:
    \begin{align*}
        &d^\dagger[1] = 4, N^\dagger[1] = \set{1, 4, 5, 7} \\
        &d^\dagger[2] = 2, N^\dagger[1] = \set{2, 6} \\
        &d^\dagger[3] = 1, N^\dagger[1] = \set{3}
    \end{align*}

    \begin{figure}
        \begin{center}
            \begin{tikzpicture}[->, >=stealth, auto, semithick, edge_style/.style={color=red, dashed}]
                % Set the positions of the nodes
                \node[circle, draw=blue, fill=blue!20, inner sep=0pt] (b1) at (1.5,2.5) {$b_1$};
                \node[circle, draw=blue, fill=blue!20, inner sep=0pt] (b2) at (3.5,2.5) {$b_2$};
                \node[circle, draw=blue, fill=blue!20, inner sep=0pt] (b3) at (5.5,2.5) {$b_3$};

                \node[circle, draw=green, fill=green!20, inner sep=0pt] (c1) at (0,0) {$c_1$};
                \node[circle, draw=green, fill=green!20, inner sep=0pt] (c2) at (1,0) {$c_2$};
                \node[circle, draw=green, fill=green!20, inner sep=0pt] (c3) at (2,0) {$c_3$};
                \node[circle, draw=green, fill=green!20, inner sep=0pt] (c4) at (3,0) {$c_4$};
                \node[circle, draw=green, fill=green!20, inner sep=0pt] (c5) at (4,0) {$c_5$};
                \node[circle, draw=green, fill=green!20, inner sep=0pt] (c6) at (5,0) {$c_6$};
                \node[circle, draw=green, fill=green!20, inner sep=0pt] (c7) at (6,0) {$c_7$};
                % Draw the edges

                \draw[edge_style] (c1) -- (b1);
                \draw[edge_style] (c2) -- (b2);
                \draw[edge_style] (c3) -- (b3);
                \draw[edge_style] (c4) -- (b1);
                \draw[edge_style] (c4) -- (b2);
                \draw[edge_style] (c5) -- (b1);
                \draw[edge_style] (c5) -- (b3);
                \draw[edge_style] (c6) -- (b2);
                \draw[edge_style] (c6) -- (b3);
                \draw[edge_style] (c7) -- (b1);
                \draw[edge_style] (c7) -- (b2);
                \draw[edge_style] (c7) -- (b3);
                % Position the parts
            \end{tikzpicture}
        \end{center}
        \caption{
            \picod
            مثالی برای درجه موثر و رأس‌های موثر
        }
        \label{figure:example:effective}

    \end{figure}
\end{example}

حال به توصیف گام‌های الگوریتم می‌پردازیم.
\begin{enumerate}
    \item{[مرتب سازی]}
    در این مرحله پیام‌ها را به ترتیب مورد نظر مرتب می‌کنیم.
    \begin{enumerate}
        \item{[گام اول]}
        در گراف اصلی که با
        $G_1$
        نشان می‌دهیم رأس پیام
        $j_1$
        با درجه بیشینه را در نظر بگیرید. در صورت وجود چند گزینه یکی را به دلخواه انتخاب کنید. خواهیم داشت
        $$\forall j \in [m] \setminus \set{j_1} \card{N[j_1]} \geq \card{N[j]}$$
        \item{[گام دوم]}
        زیر گراف القا شده توسط رأس‌های پیام
        $[m] \setminus \set{j_1}$
        و رأس‌های گیرنده‌ی
        $[n] \setminus N[j_1]$
        را با
        $G_2$
        نمایش می‌دهیم. حال مشابه گام قبل رأس پیام
        $j_2$
        با درجه بیشینه را در نظر می‌گیریم. خواهیم داشت:
        $$\forall j \in [m] \setminus \set{j_1, j_2} \card{N_{G_2}[j_2]} \geq \card{N_{G_2}[j]}$$
        که
        $N_{G_2}[j] = N[j] \cap V(G_2) = N[j] \setminus N[j_1]$
        \item{[
            گام $l$ ام
            ]}
        در گام
        $l = 3, \ldots, m$
        گراف القا شده توسط رأس‌های پیام
        $[m] \setminus \set{j_1, \ldots, j_{l - 1}}$
        و رأس‌های گیرنده‌ی
        $[n] \setminus N[j_1, \ldots, j_{l-1} ]$
        را با
        $G_l$
        نمایش می‌دهیم. رأس پیام
        $j_l$
        را مشابه قدم‌های قبل با بیشنه درجه در زیر گراف
        $G_l$
        انتخاب می‌کنیم. خواهیم داشت:
        $$\forall j \in [m] \setminus \set{j_1, \ldots, j_l}: \card{N_{G_l}[j_l]} \geq \card{N_{G_l} [j]}$$
        که
        $N_{G_l} = N[j] \cap V(G_l) = N[j] \setminus N[J_1, \ldots, j_{l - 1}] $

        اگر جایگشت انتخابی رأس‌های ما طبق فرایند بالا باشد درجه موثر
        $j_1$
        برابر
        $\card{N[j_1]}$
        خواهید بود. به طور کلی درجه اثر گذاری رأس
        $j_l$
        برابر
        $\card{N[j_l]]\setminus N[j_1, \ldots, j_{l-1}]}$
        خواهد بود.

        پس از انتخاب جایگشت به روش بالا، رأس‌ها را بر اساس درجه موثر آن‌ها به
        $\log(n)$
        دسته
        $\mathcal{M}_1, \ldots, \mathcal{M}_{\log(n)}$
        افراز می‌کنیم به گونه‌ای که برای رأس
        $j \in \mathcal{M}_s$
        داشته باشیم:
        $\dfrac{n}{2^{s - 1}} \geq d^\dagger[j] > \dfrac{n}{2^s}$

        در این صورت برای پیام
        $j$
        در دسته‌ی
        $\mathcal{M}_s$
        خواهیم داشت:
        $$d^{\dagger}[j] > \dfrac{n}{2^s} \triangleq \dfrac{d}{2}$$
        و همچنین:
        $$\card{N[j] \cap \mathcal{N}_s} \leq \dfrac{n}{2^{s - 1}} \triangleq d$$
        که
        $\mathcal{N}_s$
        مجموعه‌ی تمام گیرنده‌های موثر پیام‌های مجموعه‌ی
        $\mathcal{M}_s$
        است یعنی:
        $\mathcal{N}_s = \cup_{j' \in \mathcal{M}_s} N^{\dagger}[j']$.

        نامساوی دوم به این دلیل برقرار است که اگر
        $\card{N[j] \cap \mathcal{N}_s } > d$، در این صورت پیام
        $j'$
        در فرایند گروه بندی بالا، درجه‌ی موثر بزرگتر از
        $d$
        داشته است و در نتیجه باید در یک گروه قبل‌تر(با
        $s$
        کوچک‌تر) دسته بندی می‌شد.

        در مثال در
        \autoref{figure:example:effective}
        یک ترتیب و گروه‌بندی معتبر برابر است با
        $\seq{b_1, b_2, b_3}$
        و
        $\mathcal{M}_1 = \set{1}, \mathcal{M}_2 = \set{2}, \mathcal{M}_3 = \set{3}$
    \end{enumerate}
    \item{[مرحله ارسال]}
    برای هر گروه
    $\mathcal{M}_s$
    با استفاده از یک زیرماتریس کدگذاری با دو ردیف، دو ارسال انجام می‌دهیم.

    در ابتدا زیرماتریس خالی است. به ترتیب رأس‌های پیام
    $\mathcal{M}_s$
    را با همان ترتیب مرحله‌ی قبل بررسی می‌کنیم و به ازای هر کدام یک ستون به زیر ماتریس اضافه می‌کنیم که به آن بردار کدگذاری می‌گوییم. بردارهای کدگذاری به یکی از سه حالت
    $\set{(1, 0)^T, (0, 1)^T, (1, 1)^T}$
    هستند. در هر گام یکی از این سه بردار را بر این اساس انتخاب می‌کنیم که بیشترین تعداد گیرنده‌های
    $\mathcal{N}_s$
    را همچنان قابل ارضا باشند. در مثال
    \autoref{figure:example:effective}
    می‌توانیم زیرماتریس‌های کدگذاری زیر را بسازیم:
    \begin{align*}
        A_1 = \begin{bmatrix}
                  1 & 0 & 0 \\
                  0 & 0 & 0
        \end{bmatrix},
        A_2 =   \begin{bmatrix}
                    0 & 1 & 0 \\
                    0 & 0 & 0
        \end{bmatrix},
        A_3 =   \begin{bmatrix}
                    0 & 0 & 1 \\
                    0 & 0 & 0
        \end{bmatrix}
    \end{align*}
    که از کنار هم قرار دادن این زیر ماتریس‌ها به ماتریس کدگذاری زیر می‌رسیم:
    \begin{align*}
        A =  \begin{bmatrix}
                 A_1 \\
                 A_2 \\
                 A_3
        \end{bmatrix}
        = \begin{bmatrix}
              1 & 0 & 0 \\
              0 & 0 & 0 \\
              0 & 1 & 0 \\
              0 & 0 & 0 \\
              0 & 0 & 1 \\
              0 & 0 & 0
        \end{bmatrix}
    \end{align*}
\end{enumerate}

\subsection{شبه کد}
\begin{algorithm}[H]
    \caption[
        الگوریتم حریصانه برای میدان دودویی
        \lr{BinGreedy}
    ]{
        الگوریتم حریصانه برای میدان دودویی
        \lr{BinGreedy($G,m,n,t$)}
        \cite{pliable2016}
    }
    \label{algorithm:bingready}
    \begin{algorithmic}[1]
        \Require
        گراف اطلاعات جانبی
        $G$
        با
        $n$
        گیرنده و
        $m$
        پیام برای مسئله‌ی
        \Ensure
        یک اندیس کد منعطف برای
        $G$
        \picodtt{t}
        \State
        قرار دهید
        $\mathcal{N} = [n]$
        \State
        قرار دهید
        $\mathcal{C} = \set{}$
        \State
        برای هر
        $\forall i \in [n]$
        قرار دهید
        $CNT[i] = t$
        \While{که
            $i$
            وجود دارد که
            $CNT[i] \neq 0$
        }
            \State قرار دهید
            $B \leftarrow \varnothing$.
            \While{$B$
                یک مجموعه‌ی ماکسیمال نیست
            }
                \State
                رأس پیام
                $b_v \notin B$
                را پیدا کن به طوری که
                $\card{W_1\left(B \cup\set{b_v}\right)}$
                بیشینه شود.
                \State قرار دهید
                $B \leftarrow B \cup\set{b_v}$
            \EndWhile
            \State قرار دهید
            $\mathcal{C} \leftarrow \mathcal{C} \cup\set{\sum_{u=1}^{\card{B}} b_{v_u}, b_{v_u} \in B}$
            \For{ $c_i \in W_1(B)$ }
                \If{ $c_i$
                    به
                    $b_j \in B$
                    متصل است
                }
                    \State
                    یال متناظر در
                    $G$
                    را حذف کن.
                \EndIf
                \State
                قرار دهید
                $CNT[i] \leftarrow CNT[i]-1$.
            \EndFor
        \EndWhile
        \State کد
        $\mathcal{C}$
        را به عنوان خروجی برگردان.
    \end{algorithmic}
\end{algorithm}


\subsection{
    کارایی الگوریتم
}
\begin{lemma}
    \label{lemma:pliable2016:lemma2}
    در
    \autoref{algorithm:bingready}
    در هر گام حداقل
    $\dfrac{1}{3}$
    گیرنده‌های موثر
    $\mathcal{N}_s$
    ارضا می‌شوند.
\end{lemma}
\begin{proof}
    اثبات مفصل در مقاله آمده است. از آنجایی که این اثبات تأثیری در نتایج این پایان‌نامه ندارد از آن گذر می‌کنیم.
\end{proof}

\begin{theorem}
    \label{theorem:pliable2016:theorem1}
    در
    \autoref{algorithm:bingready}
    تعداد ارسال‌های مورد نیاز حداکثر
    $\dfrac{2}{\log(1.5)} \log^2(n)$
    است.
\end{theorem}
\begin{proof}
    طبق
    \autoref{lemma:pliable2016:lemma2}
    در هر تکرار، حداکثر
    $\log(n)$
    دسته و
    $2 * \log(n)$
    ارسال داریم که حداقل
    $1/3$
    گیرنده‌ها را ارضا می‌کنند. در نتیجه این فرایند حداکثر
    $\dfrac{\log(n)}{\log(1.5)}$
    بار می‌تواند تکرار شد.
\end{proof}

از روی پیاده‌سازی الگوریتم مشخص است که زمان اجرای آن
$O(nm^2 \log(n))$
است.
\subsection{
    ضریب تقریب
}
\begin{definition}[ضریب تقریب]
    اگر برای نمونه‌ی
    $I_n$
    از
    \picod
    طول کوتاه ترین کد را با
    $OPT(I_n)$
    نشان دهیم، ضریب تقریب الگوریتم را به صورت
    \begin{align}
        \alpha(n) = \max\limits_{I_n} \dfrac{BinGreedy(I_n)}{OPT(I_n)}
    \end{align}
    تعریف می‌کنیم.
\end{definition}


\begin{theorem}
    \label{theorem:pliable2016:theorem2}
    ضریب تقریب الگوریتم،
    $\alpha(n)$
    در نامساوی
    $$\Omega(\log \log (n)) \leq \alpha(n) \leq O(\log^2(n))$$
    صدق می‌کند مگر آنکه
    $NP \subseteq BPTIME(n^{O(\log \log (n))})$
\end{theorem}
\begin{proof}
    اثبات این قضیه در
    \cite{song2016deterministic}
    آمده است. این اثبات مشابه اثبات
    \cite{ELKIN20048}
    از
    \transf{کاهش شکاف}{gap reduction}
    از یک مسئله‌ی
    \nphard
    (
    \transf{
        مسئله‌ی نمایش‌پذیری
    }{representation problem}
    ) استفاده می‌کند و نشان می‌دهد که به دست آوردن تقریبی برای
    \picod
    با طول کمتر از
    $\Omega(\log \log (n))$
    سخت است.
\end{proof}

\subsection{
    پیرامون کد بهینه برای
    \picod
}
در این بخش با ارائه‌ی یک مثال نشان می‌دهیم که استفاده از میدان دودویی
$\mathbb{F}_2$
همیشه به کد بهینه نمی‌انجامد. همچنین ارتباط
\picod
و مسئله‌ی رتبه‌ی کمینه را بررسی می‌کنیم. به این ارتباط در بخش بعدی که بررسی نتایج عددی است نیاز داریم.
\begin{example}
    \picod
    با شرایط
    $m = 4$
    و
    $n = 10$
    را در نظر بگیرید. اطلاعات جانبی گیرنده‌ها شامل تمام زیرمجوعه‌های یک و دو عضوی از
    $[4]$
    هستند یعنی:
    $R_1 = \set{1}, R_2 = \set{2}, R_3 = \set{3}, R_4 = \set{4}, R_5 = \set{1, 2}, R_6 = \set{1, 3}, R_7 = \set{1, 4}, R_8 = \set{2, 3}, R_9 = \set{2, 4}, R_{10} = \set{3, 4}$. کد
    $Y = (b_1 + b_2 + b_4, b_2+b_3+2b_4)$
    با طول دو، یک کد بهینه در
    $\mathbb{F}_3$
    برای این مسئله خواهد بود. اما هیچ کدی با طول دو در
    $\mathbb{F}_2$
    برای این مسئله وجود ندارد. در واقع همان طور که در
    \cite{song2016deterministic}
    نشان داده شده است برای رسیده به کد با طول بهینه برای تمام نمونه‌های با
    $m$
    پیام، به میدانی با حداقل
    $m - 1$
    عضو نیاز داریم.
\end{example}

در
\autoref{subsec:arya}
با مفهوم
\lr{minrank}
در
\autoref{def:minrank}
آشنا می‌شویم و در
\autoref{thm:minranl}
نشان می‌دهیم که که طول کد بهینه در
\icod
برابر
$\minrank$
است. برای
\picod
به طور مشابه می‌توان مفهوم
$\minrank$
را به شکل زیر تعریف کرد.
\begin{definition}[مطابقت ماتریس بر \picods]
    ماتریس
    $G \in \mathbb{F}^{n\times m}_q$
    بر نمونه‌ی
    $(m, n, \set{R_i}_{i \in [m]})$
    از مسئله‌ی
    \picod
    مطابقت می‌کند اگر برای هر
    $i \in [n]$
    درسطر
    $i$-ام داشته باشیم:
    \begin{itemize}
        \item
        برای هر
        $j \in R_i$
        دقیقا یک
        $j^\ast \in R_i$
        وجود داشته باشد که
        $g_{i j^\ast}= 1$
        و برای هر
        $h \in R_i \setminus \set{j^\ast}$
        داشته باشیم:
        $g_{i j} = 0$
        \item
        برای هر
        $j \in S_i$
        مقدار
        $g_{i j}$
        هر کدام از اعضای میدان
        $\mathbb{F}_q$
        می‌تواند باشد.
    \end{itemize}
\end{definition}
\begin{definition}[رتبه‌کمینه \picods]
    اگر
    $\mathcal{G}$
    را مجموعه‌ی تمام ماتریس‌هایی که بر گراف اطلاعات جانبی    $G$ در مسئله‌ی
    \picod
    مطابقت می‌کنند بگیریم، رتبه‌کمینه‌ی مسئله را به شکل زیر تعریف می‌کنیم:
    $$minrank(\mathcal{G}) = \min_{H \in \mathcal{G}} \rank(H)$$

\end{definition}
\begin{theorem}
    \label{theorem:pliable2016:theorem3}
    طول کد خطی بهینه برای نمونه‌ی
    $(m, n, \set{R_i}_{i \in [n]})$
    ،
    \picod
    برابر
    $minrank(\mathcal{G})$
    است.
\end{theorem}
\begin{proof}
    اثبات مفصل در مقاله بعدی فرگولی
    \cite{song2016deterministic}
    آمده است.
\end{proof}

\subsection{
    نتایج عددی
}
\subsubsection{
    مقایسه کارایی
}
برای مقایسه‌ی کارایی، کارایی این الگوریتم را با الگوریتم تصادفی‌ای که پیش از این معرفی کردیم و \transf{بهترین}{state-of-the art} الگوریتم موجود است و می‌تواند به صورت میانگین کدی با طول
$O(\log^2(n))$
تولید کند، مقایسه می‌کنیم. برای این مقایسه مقدار
$m$
را برابر
$n^{0.75}$
قرار می‌دهیم. ۱۰۰ نمونه تصادفی برای هر مقدار
$n$
می‌سازیم و هر پیام را با احتمال
$0.3$
در مجوعه‌ی اطلاعات جانبی هر گیرنده قرار می‌دهیم.

\autoref{fig:pliable20162a}
میانگین طول کد و بدترین طول کد به ازای
$n$
های مختلف نشان می‌دهد. محور افقی به صورت لگاریتمی مقیاس بندی شده است. همان طور که در نمودار مشخص است، در معیار بدترین طول کد حدود
$40\%-60\%$
و در معیار میانگین طول کد الگوریتم
$20\%-35\%$
الگوریتم
\lr{BinGreedy}
نسبت به الگوریتم تصادفی بهتر عمل می‌کند.
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figs/ch3/pliable2016_2a}
        \caption{}
        \label{fig:pliable20162a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figs/ch3/pliable2016_2b}
        \caption{
        }
        \label{fig:pliable20162b}
    \end{subfigure}
    \caption{
        مقایسه‌ی
        \lr{BinGreedy}
        و الگوریتم تصادفی
    }
\end{figure}

\subsubsection{
    فاصله با جواب بهینه
}
برای بررسی کارایی الگوریتم
\lr{BinGreedy}
طول کد آن را طبق
\autoref{theorem:pliable2016:theorem3}
با طول کد بهینه که از طریق محاسبه‌ی رتبه‌کمینه‌ی مسئله به دست می‌آید مقایسه می‌کنیم. به دلیل سختی این محاسبه، مقایسه را تنها برای مقادیر کوچک انجام می‌دهیم. این مقایسه را برای مقدارهای
$n = 12$
و
$n = 18$
به ازای مقدارهای مختلف
$m$
انجام می‌دهیم. در
\lr{BinGreedy}
یک گام اضافه نیز برای بهبود طول کد اضافه می‌کنیم به این صورت که یک پایه برای فضای سطری ماتریس کدگذاری نگه می‌داریم. برای هر جفت
$m$
و
$n$
به صورت تصادفی ۵ گراف دوبخشی می‌سازیم که بین هر گیرنده و فرستنده با احتمال
$0.3$
یال هست.

همان طور که در
\autoref{fig:pliable20163}
مشخص است، فاصله‌ی میانگین(خط مشکی) برای هر دو حالت
$n = 12$
و
$n = 18$
برابر
$2$
است. بیشینه‌ی فاصله(خط سفید) نیز برای هر دوی این حالت‌ها
$3$
است.

ضریب تقریب برای حالت
$n = 18$
برابر
$2.01$
و برای
$n = 12$
برابر
$1.87$
است که با شهود ما از
\autoref{theorem:pliable2016:theorem2}
مطابقی می‌کند. زیرا ضریب تقریب نمی‌تواند بهتر از
$\Omega(\log \log (n))$
باشد و در نتیجه با افزایش
$n$
افزایش پیدا می‌کند.
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figs/ch3/pliable2016_3a}
        \caption{}
        \label{fig:pliable20163a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figs/ch3/pliable2016_3b}
        \caption{}
        \label{fig:pliable20163b}
    \end{subfigure}
    \caption{
        فاصله‌ی
        \lr{BinGreedy}
        و جواب بهینه
    }
    \label{fig:pliable20163}
\end{figure}








